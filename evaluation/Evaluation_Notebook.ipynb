{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396d0bcc",
   "metadata": {},
   "source": [
    "## üìã Setup & Configuration\n",
    "\n",
    "First, let's configure the paths and parameters for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "TEST_IMAGES_DIR = Path(\"test_data\")  # Change this to your test images directory\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "CUSTOM_OUTPUT = OUTPUT_DIR / \"custom_method\"\n",
    "DIFFEDIT_OUTPUT = OUTPUT_DIR / \"diffedit\"\n",
    "\n",
    "# Number of test images to use (set to None to use all)\n",
    "MAX_TEST_IMAGES = 20  # Adjust based on your needs\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\"  # Change to \"cpu\" if no GPU available\n",
    "\n",
    "# Create output directories\n",
    "CUSTOM_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "DIFFEDIT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Configuration set\")\n",
    "print(f\"  Test images: {TEST_IMAGES_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Max images: {MAX_TEST_IMAGES if MAX_TEST_IMAGES else 'All'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f278a",
   "metadata": {},
   "source": [
    "## üîç Understanding the Evaluation Metrics\n",
    "\n",
    "### How Mask Quality is Evaluated (IoU - Intersection over Union)\n",
    "\n",
    "**Without Ground Truth Masks:**\n",
    "Since we don't have manual ground truth masks, we use **cross-validation**:\n",
    "1. We compare the masks generated by both methods\n",
    "2. Higher IoU with the original segmentation = better consistency\n",
    "3. We also evaluate the **quality of the final output** (FID, IS, CLIP)\n",
    "\n",
    "**The key insight:** A good mask should:\n",
    "- Accurately segment the vehicle\n",
    "- Lead to better final image quality\n",
    "- Be consistent across the pipeline\n",
    "\n",
    "### Metric Details:\n",
    "\n",
    "1. **FID (Fr√©chet Inception Distance)** ‚¨áÔ∏è Lower is Better\n",
    "   - Compares distribution of generated images to real images\n",
    "   - Uses deep features from Inception network\n",
    "   - **Good**: < 50, **Excellent**: < 30\n",
    "   - Measures: Overall image quality and realism\n",
    "\n",
    "2. **IS (Inception Score)** ‚¨ÜÔ∏è Higher is Better\n",
    "   - Measures quality and diversity of generated images\n",
    "   - **Good**: > 3.0, **Excellent**: > 4.0\n",
    "   - Formula: exp(E[KL(p(y|x) || p(y))])\n",
    "   - High score = diverse, high-quality images\n",
    "\n",
    "3. **IoU (Intersection over Union)** ‚¨ÜÔ∏è Higher is Better\n",
    "   - Measures mask overlap: IoU = (A ‚à© B) / (A ‚à™ B)\n",
    "   - **Good**: > 0.7, **Excellent**: > 0.85\n",
    "   - Compares generated masks between methods\n",
    "   - Higher = more accurate segmentation\n",
    "\n",
    "4. **LPIPS (Learned Perceptual Image Patch Similarity)** ‚¨áÔ∏è Lower is Better\n",
    "   - Uses deep learning to measure perceptual similarity\n",
    "   - **Good**: < 0.3\n",
    "   - Better than pixel-wise metrics (MSE, SSIM)\n",
    "\n",
    "5. **CLIP Score** ‚¨ÜÔ∏è Higher is Better\n",
    "   - Measures how well image matches text prompt\n",
    "   - **Good**: > 0.75, **Excellent**: > 0.85\n",
    "   - Uses CLIP's vision-language alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745f57b",
   "metadata": {},
   "source": [
    "## üìù Step 1: Prepare Test Images & Prompts\n",
    "\n",
    "Let's check what test images we have and create prompts for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Check test images\n",
    "if not TEST_IMAGES_DIR.exists():\n",
    "    print(f\"‚ùå Test images directory not found: {TEST_IMAGES_DIR}\")\n",
    "    print(\"Please create it and add test images, or update TEST_IMAGES_DIR\")\n",
    "else:\n",
    "    image_files = sorted([\n",
    "        f for f in TEST_IMAGES_DIR.iterdir()\n",
    "        if f.suffix.lower() in ['.jpg', '.jpeg', '.png']\n",
    "    ])\n",
    "    \n",
    "    if MAX_TEST_IMAGES:\n",
    "        image_files = image_files[:MAX_TEST_IMAGES]\n",
    "    \n",
    "    print(f\"‚úì Found {len(image_files)} test images\")\n",
    "    \n",
    "    # Display first few images\n",
    "    if len(image_files) > 0:\n",
    "        print(\"\\nFirst few test images:\")\n",
    "        for i, img_file in enumerate(image_files[:5], 1):\n",
    "            print(f\"  {i}. {img_file.name}\")\n",
    "        if len(image_files) > 5:\n",
    "            print(f\"  ... and {len(image_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7aadf3",
   "metadata": {},
   "source": [
    "### Create Prompts\n",
    "\n",
    "You need to create prompts for each test image. Here are two approaches:\n",
    "\n",
    "**Option A: Use the sample prompts below and customize**\n",
    "**Option B: Load from existing JSON files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7562ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompts - CUSTOMIZE THESE for your specific images\n",
    "sample_prompts_custom = {\n",
    "    # Format: \"filename.jpg\": {\"vehicle\": \"description\", \"background\": \"description\"}\n",
    "}\n",
    "\n",
    "sample_prompts_diffedit = {\n",
    "    # Format: \"filename.jpg\": {\"source\": \"what it is\", \"target\": \"what you want\"}\n",
    "}\n",
    "\n",
    "# Auto-generate generic prompts if you have images but no prompts\n",
    "# (You should replace these with specific prompts for better results)\n",
    "if len(image_files) > 0 and len(sample_prompts_custom) == 0:\n",
    "    print(\"Creating generic prompts for all images...\")\n",
    "    for img_file in image_files:\n",
    "        img_name = img_file.name\n",
    "        # Generic prompts - PLEASE CUSTOMIZE FOR YOUR IMAGES\n",
    "        sample_prompts_custom[img_name] = {\n",
    "            \"vehicle\": \"sleek modern sports car\",\n",
    "            \"background\": \"scenic highway at sunset\"\n",
    "        }\n",
    "        sample_prompts_diffedit[img_name] = {\n",
    "            \"source\": \"a car on the road\",\n",
    "            \"target\": \"a modern sports car\"\n",
    "        }\n",
    "    \n",
    "    print(f\"‚úì Created generic prompts for {len(image_files)} images\")\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Using generic prompts!\")\n",
    "    print(\"   For best results, create specific prompts for each image.\")\n",
    "    print(\"   Edit the cell above with your custom prompts.\")\n",
    "\n",
    "# Save prompts to files\n",
    "prompts_custom_file = Path(\"prompts_custom.json\")\n",
    "prompts_diffedit_file = Path(\"prompts_diffedit.json\")\n",
    "\n",
    "with open(prompts_custom_file, 'w') as f:\n",
    "    json.dump(sample_prompts_custom, f, indent=2)\n",
    "\n",
    "with open(prompts_diffedit_file, 'w') as f:\n",
    "    json.dump(sample_prompts_diffedit, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Prompts saved to:\")\n",
    "print(f\"  - {prompts_custom_file}\")\n",
    "print(f\"  - {prompts_diffedit_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb551106",
   "metadata": {},
   "source": [
    "## üöÄ Step 2: Run Custom Method\n",
    "\n",
    "This will process all test images through your 4-stage pipeline:\n",
    "1. **Stage 1**: UNet segmentation (original image)\n",
    "2. **Stage 2**: Stable Diffusion vehicle regeneration\n",
    "3. **Stage 3**: UNet re-segmentation (edited image)\n",
    "4. **Stage 4**: Stable Diffusion background inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e61560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom pipeline functions\n",
    "from main import (\n",
    "    segment_image,\n",
    "    regenerate_vehicle,\n",
    "    segment_edited_image,\n",
    "    inpaint_background,\n",
    "    BASE_DIR\n",
    ")\n",
    "\n",
    "# Model paths\n",
    "unet_path = BASE_DIR / \"model\" / \"unet_model_carvana_new.pth\"\n",
    "sd_model_path = (\n",
    "    BASE_DIR / \"model\" / \"stable-diffusion\" / \n",
    "    \"models--runwayml--stable-diffusion-v1-5\" / \"snapshots\" /\n",
    "    \"451f4fe16113bff5a5d2269ed5ad43b0592e9a14\"\n",
    ")\n",
    "\n",
    "# Check models exist\n",
    "if not unet_path.exists():\n",
    "    print(f\"‚ùå UNet model not found: {unet_path}\")\n",
    "    print(\"Please run setup.py to download models\")\n",
    "else:\n",
    "    print(f\"‚úì UNet model found\")\n",
    "\n",
    "if not sd_model_path.exists():\n",
    "    print(f\"‚ùå Stable Diffusion model not found: {sd_model_path}\")\n",
    "    print(\"Please run setup.py to download models\")\n",
    "else:\n",
    "    print(f\"‚úì Stable Diffusion model found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images with custom method\n",
    "print(\"Processing images with Custom Method...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "custom_results = []\n",
    "\n",
    "for img_file in tqdm(image_files, desc=\"Custom Method\"):\n",
    "    img_name = img_file.name\n",
    "    \n",
    "    if img_name not in sample_prompts_custom:\n",
    "        print(f\"‚ö† No prompts for {img_name}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    prompts = sample_prompts_custom[img_name]\n",
    "    vehicle_prompt = prompts.get('vehicle', '')\n",
    "    background_prompt = prompts.get('background', '')\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Initial Segmentation\n",
    "        stage1_mask_name = f\"stage1_mask_{img_name}\"\n",
    "        stage1_mask_path = segment_image(\n",
    "            img_path=str(img_file),\n",
    "            model_path=str(unet_path),\n",
    "            output_dir=CUSTOM_OUTPUT,\n",
    "            output_name=stage1_mask_name\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Vehicle Regeneration\n",
    "        stage2_vehicle_name = f\"stage2_vehicle_{img_name}\"\n",
    "        _, stage2_vehicle_path = regenerate_vehicle(\n",
    "            img_path=str(img_file),\n",
    "            mask_path=stage1_mask_path,\n",
    "            model_dir=str(sd_model_path),\n",
    "            prompt=vehicle_prompt,\n",
    "            output_dir=CUSTOM_OUTPUT,\n",
    "            output_name=stage2_vehicle_name\n",
    "        )\n",
    "        \n",
    "        # Stage 3: Re-segmentation\n",
    "        stage3_mask_name = f\"stage3_mask_{img_name}\"\n",
    "        stage3_mask_path = segment_edited_image(\n",
    "            img_path=stage2_vehicle_path,\n",
    "            model_path=str(unet_path),\n",
    "            output_dir=CUSTOM_OUTPUT,\n",
    "            output_name=stage3_mask_name\n",
    "        )\n",
    "        \n",
    "        # Stage 4: Background Inpainting\n",
    "        stage4_final_name = f\"final_{img_name}\"\n",
    "        _, stage4_final_path = inpaint_background(\n",
    "            img_path=stage2_vehicle_path,\n",
    "            mask_path=stage3_mask_path,\n",
    "            model_dir=str(sd_model_path),\n",
    "            prompt=background_prompt,\n",
    "            output_dir=CUSTOM_OUTPUT,\n",
    "            output_name=stage4_final_name\n",
    "        )\n",
    "        \n",
    "        custom_results.append({\n",
    "            'image': img_name,\n",
    "            'status': 'success',\n",
    "            'stage1_mask': stage1_mask_path,\n",
    "            'stage3_mask': stage3_mask_path,\n",
    "            'final': stage4_final_path\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing {img_name}: {e}\")\n",
    "        custom_results.append({\n",
    "            'image': img_name,\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "successful_custom = len([r for r in custom_results if r['status'] == 'success'])\n",
    "print(f\"\\n‚úì Custom method completed: {successful_custom}/{len(image_files)} images successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1d277",
   "metadata": {},
   "source": [
    "## üé® Step 3: Run DiffEdit\n",
    "\n",
    "Now let's run DiffEdit on the same images for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ff204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Setup DiffEdit pipeline\n",
    "print(\"Loading DiffEdit pipeline...\")\n",
    "diffedit_pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "diffedit_pipeline.scheduler = DDIMScheduler.from_config(diffedit_pipeline.scheduler.config)\n",
    "diffedit_pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(\n",
    "    diffedit_pipeline.scheduler.config\n",
    ")\n",
    "\n",
    "diffedit_pipeline.enable_model_cpu_offload()\n",
    "diffedit_pipeline.enable_vae_slicing()\n",
    "\n",
    "print(\"‚úì DiffEdit pipeline loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images with DiffEdit\n",
    "print(\"\\nProcessing images with DiffEdit...\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: DiffEdit includes an inversion step, so each image takes longer (~60-120s)\")\n",
    "\n",
    "diffedit_results = []\n",
    "IMAGE_SIZE = (768, 768)\n",
    "\n",
    "for img_file in tqdm(image_files, desc=\"DiffEdit\"):\n",
    "    img_name = img_file.name\n",
    "    \n",
    "    if img_name not in sample_prompts_diffedit:\n",
    "        print(f\"‚ö† No prompts for {img_name}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    prompts = sample_prompts_diffedit[img_name]\n",
    "    source_prompt = prompts.get('source', '')\n",
    "    target_prompt = prompts.get('target', '')\n",
    "    \n",
    "    try:\n",
    "        # Load and resize image\n",
    "        raw_image = load_image(str(img_file)).resize(IMAGE_SIZE)\n",
    "        \n",
    "        # Generate mask\n",
    "        mask_image = diffedit_pipeline.generate_mask(\n",
    "            image=raw_image,\n",
    "            source_prompt=source_prompt,\n",
    "            target_prompt=target_prompt,\n",
    "        )\n",
    "        \n",
    "        # Invert latents\n",
    "        inv_latents = diffedit_pipeline.invert(\n",
    "            prompt=source_prompt,\n",
    "            image=raw_image\n",
    "        ).latents\n",
    "        \n",
    "        # Generate final image\n",
    "        output_image = diffedit_pipeline(\n",
    "            prompt=target_prompt,\n",
    "            mask_image=mask_image,\n",
    "            image_latents=inv_latents,\n",
    "            negative_prompt=source_prompt,\n",
    "        ).images[0]\n",
    "        \n",
    "        # Save outputs\n",
    "        output_path = DIFFEDIT_OUTPUT / f\"edited_{img_name}\"\n",
    "        output_image.save(output_path)\n",
    "        \n",
    "        # Save mask\n",
    "        mask_pil = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\")\n",
    "        mask_pil = mask_pil.resize(IMAGE_SIZE)\n",
    "        mask_path = DIFFEDIT_OUTPUT / f\"mask_{img_name}\"\n",
    "        mask_pil.save(mask_path)\n",
    "        \n",
    "        diffedit_results.append({\n",
    "            'image': img_name,\n",
    "            'status': 'success',\n",
    "            'output': str(output_path),\n",
    "            'mask': str(mask_path)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing {img_name}: {e}\")\n",
    "        diffedit_results.append({\n",
    "            'image': img_name,\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "successful_diffedit = len([r for r in diffedit_results if r['status'] == 'success'])\n",
    "print(f\"\\n‚úì DiffEdit completed: {successful_diffedit}/{len(image_files)} images successful\")\n",
    "\n",
    "# Free memory\n",
    "del diffedit_pipeline\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bad656",
   "metadata": {},
   "source": [
    "## üìä Step 4: Compute Evaluation Metrics\n",
    "\n",
    "Now let's compute all the metrics to compare both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "print(\"Initializing evaluation metrics...\")\n",
    "\n",
    "# Initialize metrics\n",
    "fid_metric = FrechetInceptionDistance(normalize=True).to(DEVICE)\n",
    "lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(DEVICE)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "print(\"‚úì Metrics initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for metric computation\n",
    "\n",
    "def preprocess_for_fid(image_path):\n",
    "    \"\"\"Convert image to tensor for FID\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    if img_array.dtype != np.uint8:\n",
    "        img_array = (img_array * 255).astype(np.uint8)\n",
    "    tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
    "    return tensor.to(DEVICE)\n",
    "\n",
    "def preprocess_for_lpips(image_path):\n",
    "    \"\"\"Convert image to tensor for LPIPS\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def calculate_clip_similarity(image_path, text_prompt):\n",
    "    \"\"\"Calculate CLIP image-text similarity\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    image_input = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "    text_input = clip.tokenize([text_prompt]).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "        \n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def calculate_iou(mask1_path, mask2_path):\n",
    "    \"\"\"Calculate IoU between two masks\"\"\"\n",
    "    mask1 = np.array(Image.open(mask1_path).convert(\"L\")) > 127\n",
    "    mask2 = np.array(Image.open(mask2_path).convert(\"L\")) > 127\n",
    "    \n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for Custom Method\n",
    "print(\"Computing metrics for Custom Method...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "custom_metrics = {\n",
    "    'lpips_scores': [],\n",
    "    'clip_scores': [],\n",
    "    'iou_scores': [],\n",
    "    'per_image': []\n",
    "}\n",
    "\n",
    "# Reset FID\n",
    "fid_metric.reset()\n",
    "\n",
    "for result in tqdm(custom_results, desc=\"Custom Metrics\"):\n",
    "    if result['status'] != 'success':\n",
    "        continue\n",
    "    \n",
    "    img_name = result['image']\n",
    "    orig_path = TEST_IMAGES_DIR / img_name\n",
    "    final_path = Path(result['final'])\n",
    "    \n",
    "    # FID\n",
    "    orig_tensor = preprocess_for_fid(orig_path)\n",
    "    final_tensor = preprocess_for_fid(final_path)\n",
    "    fid_metric.update(orig_tensor, real=True)\n",
    "    fid_metric.update(final_tensor, real=False)\n",
    "    \n",
    "    # LPIPS\n",
    "    orig_lpips = preprocess_for_lpips(orig_path)\n",
    "    final_lpips = preprocess_for_lpips(final_path)\n",
    "    lpips_score = lpips_metric(final_lpips, orig_lpips).item()\n",
    "    custom_metrics['lpips_scores'].append(lpips_score)\n",
    "    \n",
    "    # CLIP (using target prompt)\n",
    "    if img_name in sample_prompts_custom:\n",
    "        prompts = sample_prompts_custom[img_name]\n",
    "        combined_prompt = f\"{prompts['vehicle']} on {prompts['background']}\"\n",
    "        clip_score = calculate_clip_similarity(final_path, combined_prompt)\n",
    "        custom_metrics['clip_scores'].append(clip_score)\n",
    "    \n",
    "    # IoU (comparing stage 1 and stage 3 masks for consistency)\n",
    "    iou_score = calculate_iou(result['stage1_mask'], result['stage3_mask'])\n",
    "    custom_metrics['iou_scores'].append(iou_score)\n",
    "    \n",
    "    custom_metrics['per_image'].append({\n",
    "        'image': img_name,\n",
    "        'lpips': lpips_score,\n",
    "        'clip': clip_score if img_name in sample_prompts_custom else None,\n",
    "        'iou': iou_score\n",
    "    })\n",
    "\n",
    "# Compute FID\n",
    "custom_metrics['fid'] = fid_metric.compute().item()\n",
    "custom_metrics['avg_lpips'] = np.mean(custom_metrics['lpips_scores'])\n",
    "custom_metrics['avg_clip'] = np.mean(custom_metrics['clip_scores'])\n",
    "custom_metrics['avg_iou'] = np.mean(custom_metrics['iou_scores'])\n",
    "\n",
    "print(f\"‚úì Custom Method Metrics:\")\n",
    "print(f\"  FID:   {custom_metrics['fid']:.3f}\")\n",
    "print(f\"  LPIPS: {custom_metrics['avg_lpips']:.4f}\")\n",
    "print(f\"  CLIP:  {custom_metrics['avg_clip']:.4f}\")\n",
    "print(f\"  IoU:   {custom_metrics['avg_iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for DiffEdit\n",
    "print(\"\\nComputing metrics for DiffEdit...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diffedit_metrics = {\n",
    "    'lpips_scores': [],\n",
    "    'clip_scores': [],\n",
    "    'iou_scores': [],\n",
    "    'per_image': []\n",
    "}\n",
    "\n",
    "# Reset FID\n",
    "fid_metric.reset()\n",
    "\n",
    "for result in tqdm(diffedit_results, desc=\"DiffEdit Metrics\"):\n",
    "    if result['status'] != 'success':\n",
    "        continue\n",
    "    \n",
    "    img_name = result['image']\n",
    "    orig_path = TEST_IMAGES_DIR / img_name\n",
    "    output_path = Path(result['output'])\n",
    "    \n",
    "    # FID\n",
    "    orig_tensor = preprocess_for_fid(orig_path)\n",
    "    output_tensor = preprocess_for_fid(output_path)\n",
    "    fid_metric.update(orig_tensor, real=True)\n",
    "    fid_metric.update(output_tensor, real=False)\n",
    "    \n",
    "    # LPIPS\n",
    "    orig_lpips = preprocess_for_lpips(orig_path)\n",
    "    output_lpips = preprocess_for_lpips(output_path)\n",
    "    lpips_score = lpips_metric(output_lpips, orig_lpips).item()\n",
    "    diffedit_metrics['lpips_scores'].append(lpips_score)\n",
    "    \n",
    "    # CLIP\n",
    "    if img_name in sample_prompts_diffedit:\n",
    "        target_prompt = sample_prompts_diffedit[img_name]['target']\n",
    "        clip_score = calculate_clip_similarity(output_path, target_prompt)\n",
    "        diffedit_metrics['clip_scores'].append(clip_score)\n",
    "    \n",
    "    # IoU (compare with custom method's mask for the same image)\n",
    "    custom_result = next((r for r in custom_results if r['image'] == img_name), None)\n",
    "    if custom_result and custom_result['status'] == 'success':\n",
    "        iou_score = calculate_iou(result['mask'], custom_result['stage3_mask'])\n",
    "        diffedit_metrics['iou_scores'].append(iou_score)\n",
    "    \n",
    "    diffedit_metrics['per_image'].append({\n",
    "        'image': img_name,\n",
    "        'lpips': lpips_score,\n",
    "        'clip': clip_score if img_name in sample_prompts_diffedit else None,\n",
    "        'iou': iou_score if custom_result else None\n",
    "    })\n",
    "\n",
    "# Compute FID\n",
    "diffedit_metrics['fid'] = fid_metric.compute().item()\n",
    "diffedit_metrics['avg_lpips'] = np.mean(diffedit_metrics['lpips_scores'])\n",
    "diffedit_metrics['avg_clip'] = np.mean(diffedit_metrics['clip_scores'])\n",
    "diffedit_metrics['avg_iou'] = np.mean(diffedit_metrics['iou_scores']) if diffedit_metrics['iou_scores'] else 0\n",
    "\n",
    "print(f\"‚úì DiffEdit Metrics:\")\n",
    "print(f\"  FID:   {diffedit_metrics['fid']:.3f}\")\n",
    "print(f\"  LPIPS: {diffedit_metrics['avg_lpips']:.4f}\")\n",
    "print(f\"  CLIP:  {diffedit_metrics['avg_clip']:.4f}\")\n",
    "print(f\"  IoU:   {diffedit_metrics['avg_iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d5625",
   "metadata": {},
   "source": [
    "## üìà Step 5: Compare Results\n",
    "\n",
    "Let's create a comprehensive comparison of both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARATIVE RESULTS: Custom Method vs DiffEdit\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['FID ‚Üì', 'LPIPS ‚Üì', 'CLIP ‚Üë', 'IoU ‚Üë'],\n",
    "    'Custom Method': [\n",
    "        f\"{custom_metrics['fid']:.3f}\",\n",
    "        f\"{custom_metrics['avg_lpips']:.4f}\",\n",
    "        f\"{custom_metrics['avg_clip']:.4f}\",\n",
    "        f\"{custom_metrics['avg_iou']:.4f}\"\n",
    "    ],\n",
    "    'DiffEdit': [\n",
    "        f\"{diffedit_metrics['fid']:.3f}\",\n",
    "        f\"{diffedit_metrics['avg_lpips']:.4f}\",\n",
    "        f\"{diffedit_metrics['avg_clip']:.4f}\",\n",
    "        f\"{diffedit_metrics['avg_iou']:.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Determine winners\n",
    "winners = []\n",
    "metrics_list = [\n",
    "    ('fid', False),  # lower is better\n",
    "    ('avg_lpips', False),\n",
    "    ('avg_clip', True),  # higher is better\n",
    "    ('avg_iou', True)\n",
    "]\n",
    "\n",
    "for metric_key, higher_better in metrics_list:\n",
    "    custom_val = custom_metrics[metric_key]\n",
    "    diffedit_val = diffedit_metrics[metric_key]\n",
    "    \n",
    "    if higher_better:\n",
    "        winner = '‚úì Custom' if custom_val > diffedit_val else '‚úì DiffEdit'\n",
    "    else:\n",
    "        winner = '‚úì Custom' if custom_val < diffedit_val else '‚úì DiffEdit'\n",
    "    \n",
    "    winners.append(winner)\n",
    "\n",
    "df['Winner'] = winners\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Count wins\n",
    "custom_wins = winners.count('‚úì Custom')\n",
    "diffedit_wins = winners.count('‚úì DiffEdit')\n",
    "\n",
    "print(f\"\\nOVERALL WINNER: \", end=\"\")\n",
    "if custom_wins > diffedit_wins:\n",
    "    print(\"‚úì Custom Method\")\n",
    "elif diffedit_wins > custom_wins:\n",
    "    print(\"‚úì DiffEdit\")\n",
    "else:\n",
    "    print(\"Tie\")\n",
    "\n",
    "print(f\"(Custom: {custom_wins} wins, DiffEdit: {diffedit_wins} wins)\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea5c0f",
   "metadata": {},
   "source": [
    "## üìä Step 6: Visualize Results\n",
    "\n",
    "Let's create visual comparisons of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59883c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('FID', 'fid', False),\n",
    "    ('LPIPS', 'avg_lpips', False),\n",
    "    ('CLIP', 'avg_clip', True),\n",
    "    ('IoU', 'avg_iou', True)\n",
    "]\n",
    "\n",
    "for idx, (name, key, higher_better) in enumerate(metrics_to_plot):\n",
    "    custom_val = custom_metrics[key]\n",
    "    diffedit_val = diffedit_metrics[key]\n",
    "    \n",
    "    # Determine colors\n",
    "    if higher_better:\n",
    "        colors = ['green' if custom_val > diffedit_val else 'lightgreen',\n",
    "                 'blue' if diffedit_val > custom_val else 'lightblue']\n",
    "    else:\n",
    "        colors = ['green' if custom_val < diffedit_val else 'lightgreen',\n",
    "                 'blue' if diffedit_val < custom_val else 'lightblue']\n",
    "    \n",
    "    bars = axes[idx].bar(['Custom', 'DiffEdit'], \n",
    "                        [custom_val, diffedit_val],\n",
    "                        color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                      f'{height:.3f}',\n",
    "                      ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    # Styling\n",
    "    direction = '‚Üë Higher Better' if higher_better else '‚Üì Lower Better'\n",
    "    axes[idx].set_title(f'{name}\\n{direction}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score', fontsize=11)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Metrics Comparison: Custom Method vs DiffEdit', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Metrics chart saved to: {OUTPUT_DIR / 'metrics_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side image comparisons\n",
    "print(\"\\nCreating visual comparisons of individual images...\")\n",
    "\n",
    "# Select up to 5 images to display\n",
    "display_images = min(5, len([r for r in custom_results if r['status'] == 'success']))\n",
    "\n",
    "fig, axes = plt.subplots(display_images, 3, figsize=(15, 5 * display_images))\n",
    "\n",
    "if display_images == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "success_results = [(c, d) for c, d in zip(custom_results, diffedit_results) \n",
    "                   if c['status'] == 'success' and d['status'] == 'success']\n",
    "\n",
    "for idx, (custom_result, diffedit_result) in enumerate(success_results[:display_images]):\n",
    "    img_name = custom_result['image']\n",
    "    \n",
    "    # Load images\n",
    "    original = Image.open(TEST_IMAGES_DIR / img_name)\n",
    "    custom_output = Image.open(custom_result['final'])\n",
    "    diffedit_output = Image.open(diffedit_result['output'])\n",
    "    \n",
    "    # Display\n",
    "    axes[idx, 0].imshow(original)\n",
    "    axes[idx, 0].set_title(f'Original\\n{img_name}', fontsize=10)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(custom_output)\n",
    "    axes[idx, 1].set_title('Custom Method', fontsize=10, color='green', fontweight='bold')\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    axes[idx, 2].imshow(diffedit_output)\n",
    "    axes[idx, 2].set_title('DiffEdit', fontsize=10, color='blue', fontweight='bold')\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Side-by-Side Comparison of Sample Images', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'visual_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visual comparison saved to: {OUTPUT_DIR / 'visual_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497f7df",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Results\n",
    "\n",
    "Let's save all results to a JSON file for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Compile all results\n",
    "final_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'test_configuration': {\n",
    "        'num_images': len(image_files),\n",
    "        'device': DEVICE,\n",
    "        'test_images_dir': str(TEST_IMAGES_DIR)\n",
    "    },\n",
    "    'custom_method': {\n",
    "        'fid': custom_metrics['fid'],\n",
    "        'lpips': custom_metrics['avg_lpips'],\n",
    "        'clip': custom_metrics['avg_clip'],\n",
    "        'iou': custom_metrics['avg_iou'],\n",
    "        'num_successful': len([r for r in custom_results if r['status'] == 'success'])\n",
    "    },\n",
    "    'diffedit_method': {\n",
    "        'fid': diffedit_metrics['fid'],\n",
    "        'lpips': diffedit_metrics['avg_lpips'],\n",
    "        'clip': diffedit_metrics['avg_clip'],\n",
    "        'iou': diffedit_metrics['avg_iou'],\n",
    "        'num_successful': len([r for r in diffedit_results if r['status'] == 'success'])\n",
    "    },\n",
    "    'per_image_comparison': []\n",
    "}\n",
    "\n",
    "# Add per-image details\n",
    "for custom_img, diffedit_img in zip(custom_metrics['per_image'], diffedit_metrics['per_image']):\n",
    "    final_results['per_image_comparison'].append({\n",
    "        'image': custom_img['image'],\n",
    "        'custom': custom_img,\n",
    "        'diffedit': diffedit_img\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "results_file = OUTPUT_DIR / 'evaluation_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996bb4e",
   "metadata": {},
   "source": [
    "## üéØ Summary & Interpretation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "Your custom method should outperform DiffEdit if:\n",
    "- **FID is lower** ‚Üí Better image quality\n",
    "- **IoU is higher** ‚Üí Better segmentation accuracy\n",
    "- **CLIP is higher** ‚Üí Better text-prompt alignment\n",
    "\n",
    "### What the IoU Score Actually Means\n",
    "\n",
    "Since we don't have ground truth masks, the IoU here measures:\n",
    "1. **For Custom Method**: Consistency between Stage 1 and Stage 3 masks\n",
    "   - High IoU = the segmentation is stable/consistent\n",
    "2. **Between Methods**: How similar the masks are\n",
    "   - This shows if both methods identify similar regions\n",
    "\n",
    "### Understanding \"Good\" vs \"Bad\" Masks\n",
    "\n",
    "A good mask:\n",
    "- ‚úì Cleanly separates foreground from background\n",
    "- ‚úì Follows object boundaries accurately\n",
    "- ‚úì Leads to better final image quality (reflected in FID/CLIP)\n",
    "- ‚úì Is consistent across the pipeline\n",
    "\n",
    "A bad mask:\n",
    "- ‚úó Has rough/jagged edges\n",
    "- ‚úó Misses parts of the object or includes too much background\n",
    "- ‚úó Causes artifacts in the final image\n",
    "- ‚úó Is inconsistent between stages\n",
    "\n",
    "### Why Custom Method Should Win\n",
    "\n",
    "Your 4-stage approach has advantages:\n",
    "1. **Trained segmentation** (UNet) vs heuristic mask generation\n",
    "2. **Sequential refinement** (re-segment after editing)\n",
    "3. **Specialized control** (vehicle and background separately)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Review the visual comparisons above\n",
    "2. Check the JSON file for detailed per-image metrics\n",
    "3. Identify failure cases (if any)\n",
    "4. Fine-tune prompts for better results\n",
    "5. Use these results in your report/paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb90437",
   "metadata": {},
   "source": [
    "## üìù Recommended Test Set Sizes\n",
    "\n",
    "Based on your evaluation needs:\n",
    "\n",
    "| Purpose | Images | Time (GPU) | Why |\n",
    "|---------|--------|------------|-----|\n",
    "| **Quick Test** | 5-10 | ~15-30 min | Verify pipeline works |\n",
    "| **Development** | 10-15 | ~30-45 min | Iterate on prompts |\n",
    "| **Evaluation** | 20-30 | ~1-2 hours | Reliable statistics |\n",
    "| **Publication** | 50-100 | ~3-5 hours | Publication-quality results |\n",
    "\n",
    "### Statistical Reliability\n",
    "\n",
    "- **< 10 images**: Results may be unreliable, high variance\n",
    "- **10-20 images**: Moderate confidence, good for class projects\n",
    "- **20-30 images**: Good confidence, suitable for papers\n",
    "- **50+ images**: High confidence, publication-quality\n",
    "\n",
    "The metrics become more stable with more images, especially FID and IS which measure distribution similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27416f10",
   "metadata": {},
   "source": [
    "## ‚úÖ Evaluation Complete!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úì Run both methods on your test images\n",
    "- ‚úì Computed quantitative metrics (FID, LPIPS, CLIP, IoU)\n",
    "- ‚úì Created visual comparisons\n",
    "- ‚úì Saved all results to JSON\n",
    "\n",
    "Check the `outputs/` directory for all generated images and the results JSON file."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
