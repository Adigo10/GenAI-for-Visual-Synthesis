{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "396d0bcc",
      "metadata": {
        "id": "396d0bcc"
      },
      "source": [
        "## ğŸ“‹ Setup & Configuration\n",
        "\n",
        "First, let's configure the paths and parameters for our evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b0bca76",
      "metadata": {
        "id": "9b0bca76",
        "outputId": "633bd3d7-db2a-4d42-f491-7750d3af2889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Configuration set\n",
            "  Test images: /content/GenAI-for-Visual-Synthesis/test_data\n",
            "  Output directory: /content/GenAI-for-Visual-Synthesis/outputs\n",
            "  Device: cuda\n",
            "  Max images: 75\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "TEST_IMAGES_DIR = Path(\"/content/GenAI-for-Visual-Synthesis/test_data\")  # Change this to your test images directory\n",
        "OUTPUT_DIR = Path(\"/content/GenAI-for-Visual-Synthesis/outputs\")\n",
        "CUSTOM_OUTPUT = OUTPUT_DIR / \"custom_method\"\n",
        "DIFFEDIT_OUTPUT = OUTPUT_DIR / \"diffedit\"\n",
        "\n",
        "# Number of test images to use (set to None to use all)\n",
        "MAX_TEST_IMAGES = 75  # Adjust based on your needs\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = \"cuda\"  # Change to \"cpu\" if no GPU available\n",
        "\n",
        "# Create output directories\n",
        "CUSTOM_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "DIFFEDIT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"âœ“ Configuration set\")\n",
        "print(f\"  Test images: {TEST_IMAGES_DIR}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Max images: {MAX_TEST_IMAGES if MAX_TEST_IMAGES else 'All'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa7f278a",
      "metadata": {
        "id": "aa7f278a"
      },
      "source": [
        "## ğŸ” Understanding the Evaluation Metrics\n",
        "\n",
        "### How Mask Quality is Evaluated (IoU - Intersection over Union)\n",
        "\n",
        "**Without Ground Truth Masks:**\n",
        "Since we don't have manual ground truth masks, we use **cross-validation**:\n",
        "1. We compare the masks generated by both methods\n",
        "2. Higher IoU with the original segmentation = better consistency\n",
        "3. We also evaluate the **quality of the final output** (FID, IS, CLIP)\n",
        "\n",
        "**The key insight:** A good mask should:\n",
        "- Accurately segment the vehicle\n",
        "- Lead to better final image quality\n",
        "- Be consistent across the pipeline\n",
        "\n",
        "### Metric Details:\n",
        "\n",
        "1. **FID (FrÃ©chet Inception Distance)** â¬‡ï¸ Lower is Better\n",
        "   - Compares distribution of generated images to real images\n",
        "   - Uses deep features from Inception network\n",
        "   - **Good**: < 50, **Excellent**: < 30\n",
        "   - Measures: Overall image quality and realism\n",
        "\n",
        "2. **IS (Inception Score)** â¬†ï¸ Higher is Better\n",
        "   - Measures quality and diversity of generated images\n",
        "   - **Good**: > 3.0, **Excellent**: > 4.0\n",
        "   - Formula: exp(E[KL(p(y|x) || p(y))])\n",
        "   - High score = diverse, high-quality images\n",
        "\n",
        "3. **IoU (Intersection over Union)** â¬†ï¸ Higher is Better\n",
        "   - Measures mask overlap: IoU = (A âˆ© B) / (A âˆª B)\n",
        "   - **Good**: > 0.7, **Excellent**: > 0.85\n",
        "   - Compares generated masks between methods\n",
        "   - Higher = more accurate segmentation\n",
        "\n",
        "4. **LPIPS (Learned Perceptual Image Patch Similarity)** â¬‡ï¸ Lower is Better\n",
        "   - Uses deep learning to measure perceptual similarity\n",
        "   - **Good**: < 0.3\n",
        "   - Better than pixel-wise metrics (MSE, SSIM)\n",
        "\n",
        "5. **CLIP Score** â¬†ï¸ Higher is Better\n",
        "   - Measures how well image matches text prompt\n",
        "   - **Good**: > 0.75, **Excellent**: > 0.85\n",
        "   - Uses CLIP's vision-language alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a745f57b",
      "metadata": {
        "id": "a745f57b"
      },
      "source": [
        "## ğŸ“ Step 1: Prepare Test Images & Prompts\n",
        "\n",
        "Let's check what test images we have and create prompts for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4263ed37",
      "metadata": {
        "id": "4263ed37",
        "outputId": "b9848d9c-c5ee-48c9-e660-4afc186cb2f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Found 75 test images\n",
            "\n",
            "First few test images:\n",
            "  1. 000aa097d423_03.jpg\n",
            "  2. 00ad56bf7ee6_03.jpg\n",
            "  3. 00afb946a54c_03.jpg\n",
            "  4. 00b6aee52419_03.jpg\n",
            "  5. 00c07d49f4c5_03.jpg\n",
            "  ... and 70 more\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Check test images\n",
        "if not TEST_IMAGES_DIR.exists():\n",
        "    print(f\"âŒ Test images directory not found: {TEST_IMAGES_DIR}\")\n",
        "    print(\"Please create it and add test images, or update TEST_IMAGES_DIR\")\n",
        "else:\n",
        "    image_files = sorted([\n",
        "        f for f in TEST_IMAGES_DIR.iterdir()\n",
        "        if f.suffix.lower() in ['.jpg', '.jpeg', '.png']\n",
        "    ])\n",
        "\n",
        "    if MAX_TEST_IMAGES:\n",
        "        image_files = image_files[:MAX_TEST_IMAGES]\n",
        "\n",
        "    print(f\"âœ“ Found {len(image_files)} test images\")\n",
        "\n",
        "    # Display first few images\n",
        "    if len(image_files) > 0:\n",
        "        print(\"\\nFirst few test images:\")\n",
        "        for i, img_file in enumerate(image_files[:5], 1):\n",
        "            print(f\"  {i}. {img_file.name}\")\n",
        "        if len(image_files) > 5:\n",
        "            print(f\"  ... and {len(image_files) - 5} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7aadf3",
      "metadata": {
        "id": "3e7aadf3"
      },
      "source": [
        "### Create Prompts\n",
        "\n",
        "You need to create prompts for each test image. Here are two approaches:\n",
        "\n",
        "**Option A: Use the sample prompts below and customize**\n",
        "**Option B: Load from existing JSON files**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-generate prompts using AI models\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "print(\"ğŸ¤– Loading AI models for automatic prompt generation...\")\n",
        "print(\"This may take a minute on first run (downloading models)...\\n\")\n",
        "\n",
        "# Load BLIP for image captioning\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        ").to(DEVICE)\n",
        "\n",
        "# Load FLAN-T5 for prompt enhancement\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(DEVICE)\n",
        "\n",
        "print(\"âœ“ Models loaded successfully!\\n\")\n",
        "print(\"Generating prompts...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "sample_prompts_custom = {}\n",
        "sample_prompts_diffedit = {}\n",
        "\n",
        "for img_file in tqdm(image_files, desc=\"Generating prompts\"):\n",
        "    img_name = img_file.name\n",
        "\n",
        "    try:\n",
        "        # Step 1: Get image caption using BLIP\n",
        "        image = Image.open(img_file).convert(\"RGB\")\n",
        "        inputs = blip_processor(image, return_tensors=\"pt\").to(DEVICE)\n",
        "        out = blip_model.generate(**inputs, max_length=50)\n",
        "        caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        # Step 2: Generate vehicle prompt using FLAN-T5\n",
        "        vehicle_instruction = f\"\"\"Given this image description: \"{caption}\"\n",
        "Generate a detailed, creative prompt to regenerate the vehicle in a more stylish way.\n",
        "Focus on: vehicle type, color, style, condition.\n",
        "Prompt:\"\"\"\n",
        "\n",
        "        inputs = t5_tokenizer(vehicle_instruction, return_tensors=\"pt\", max_length=512, truncation=True).to(DEVICE)\n",
        "        outputs = t5_model.generate(inputs.input_ids, max_length=100, num_beams=4, temperature=0.8, do_sample=True, top_p=0.9)\n",
        "        vehicle_prompt = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Fallback if prompt too short\n",
        "        if len(vehicle_prompt.split()) < 5:\n",
        "            vehicle_type = \"car\"\n",
        "            if \"suv\" in caption.lower():\n",
        "                vehicle_type = \"SUV\"\n",
        "            elif \"truck\" in caption.lower():\n",
        "                vehicle_type = \"truck\"\n",
        "            vehicle_prompt = f\"sleek modern {vehicle_type}, glossy finish, high detail\"\n",
        "\n",
        "        # Step 3: Generate background prompt using FLAN-T5\n",
        "        bg_instruction = f\"\"\"Given this image description: \"{caption}\"\n",
        "Generate a creative prompt for an interesting background setting.\n",
        "Focus on: environment, lighting, atmosphere, scenery.\n",
        "Prompt:\"\"\"\n",
        "\n",
        "        inputs = t5_tokenizer(bg_instruction, return_tensors=\"pt\", max_length=512, truncation=True).to(DEVICE)\n",
        "        outputs = t5_model.generate(inputs.input_ids, max_length=100, num_beams=4, temperature=0.8, do_sample=True, top_p=0.9)\n",
        "        bg_prompt = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Fallback if prompt too short\n",
        "        if len(bg_prompt.split()) < 5:\n",
        "            backgrounds = [\n",
        "                \"scenic mountain highway at golden hour\",\n",
        "                \"modern city street with glass buildings\",\n",
        "                \"coastal road with ocean view at sunset\"\n",
        "            ]\n",
        "            bg_prompt = backgrounds[hash(caption) % len(backgrounds)]\n",
        "\n",
        "        # Step 4: Generate DiffEdit prompts\n",
        "        source_instruction = f\"\"\"Simplify this description to a short phrase: \"{caption}\"\n",
        "Simplified:\"\"\"\n",
        "\n",
        "        inputs = t5_tokenizer(source_instruction, return_tensors=\"pt\", max_length=512, truncation=True).to(DEVICE)\n",
        "        outputs = t5_model.generate(inputs.input_ids, max_length=50, num_beams=2, temperature=0.3)\n",
        "        source_prompt = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if len(source_prompt.split()) < 3:\n",
        "            source_prompt = \"a car on the road\"\n",
        "\n",
        "        target_instruction = f\"\"\"Rewrite this as an enhanced, stylish version: \"{caption}\"\n",
        "Enhanced:\"\"\"\n",
        "\n",
        "        inputs = t5_tokenizer(target_instruction, return_tensors=\"pt\", max_length=512, truncation=True).to(DEVICE)\n",
        "        outputs = t5_model.generate(inputs.input_ids, max_length=50, num_beams=4, temperature=0.7, do_sample=True, top_p=0.9)\n",
        "        target_prompt = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if len(target_prompt.split()) < 3:\n",
        "            target_prompt = \"a sleek modern sports car\"\n",
        "\n",
        "        # Store prompts\n",
        "        sample_prompts_custom[img_name] = {\n",
        "            \"vehicle\": vehicle_prompt,\n",
        "            \"background\": bg_prompt,\n",
        "            \"original_caption\": caption\n",
        "        }\n",
        "\n",
        "        sample_prompts_diffedit[img_name] = {\n",
        "            \"source\": source_prompt,\n",
        "            \"target\": target_prompt,\n",
        "            \"original_caption\": caption\n",
        "        }\n",
        "\n",
        "        # Display progress\n",
        "        if len(sample_prompts_custom) <= 3:  # Show first 3\n",
        "            print(f\"\\n{img_name}\")\n",
        "            print(f\"  Caption: {caption}\")\n",
        "            print(f\"  Vehicle: {vehicle_prompt}\")\n",
        "            print(f\"  Background: {bg_prompt}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸  Error processing {img_name}: {e}\")\n",
        "        # Add fallback prompts\n",
        "        sample_prompts_custom[img_name] = {\n",
        "            \"vehicle\": \"sleek modern sports car, high detail\",\n",
        "            \"background\": \"scenic highway at sunset\"\n",
        "        }\n",
        "        sample_prompts_diffedit[img_name] = {\n",
        "            \"source\": \"a car on the road\",\n",
        "            \"target\": \"a modern sports car\"\n",
        "        }\n",
        "\n",
        "# Clean up models to free memory\n",
        "del blip_model, blip_processor, t5_model, t5_tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ“ Generated prompts for {len(sample_prompts_custom)} images!\")\n",
        "print(\"\\nSample prompts (first 3 shown above)\")\n",
        "print(\"All prompts will be saved in the next cell.\")"
      ],
      "metadata": {
        "id": "EI_62f981F_H",
        "outputId": "05babcda-c912-4738-ef70-06da9ce416cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552,
          "referenced_widgets": [
            "15f980ec1cf84beb8a4481155a9a2a93",
            "5070329658784a0ebfb59263222b1087",
            "db6e181db10b45febf123bbdaf56617d",
            "66f16b532933472e9e4636ad24c59d67",
            "ef63956b708e4d519aa31270c8fae789",
            "a195976b17924046a7b7376126a885cc",
            "f0c4182e34044597965742e4f421762b",
            "30e4227bcb0f44ee9a61570541c559ef",
            "88ca1406d3a240d8b98ce3b7c358763c",
            "2bb1447fdcb943f18747e9ae9624ff42",
            "9b832cc55e5a46b3bbba049f9adfc0b0"
          ]
        }
      },
      "id": "EI_62f981F_H",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Loading AI models for automatic prompt generation...\n",
            "This may take a minute on first run (downloading models)...\n",
            "\n",
            "âœ“ Models loaded successfully!\n",
            "\n",
            "Generating prompts...\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating prompts:   0%|          | 0/75 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15f980ec1cf84beb8a4481155a9a2a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "000aa097d423_03.jpg\n",
            "  Caption: a white car parked in a garage\n",
            "  Vehicle: A white car is parked in a garage.\n",
            "  Background: A white car parked in a garage.\n",
            "\n",
            "00ad56bf7ee6_03.jpg\n",
            "  Caption: a gray car\n",
            "  Vehicle: A gray car is in good condition.\n",
            "  Background: A gray car in a parking lot.\n",
            "\n",
            "00afb946a54c_03.jpg\n",
            "  Caption: a gray nissan rogue suv parked in a white room\n",
            "  Vehicle: a gray nissan rogue suv parked in a white room\n",
            "  Background: A gray nissan rogue suv parked in a white room.\n",
            "\n",
            "======================================================================\n",
            "âœ“ Generated prompts for 75 images!\n",
            "\n",
            "Sample prompts (first 3 shown above)\n",
            "All prompts will be saved in the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompts_custom = {}\n",
        "sample_prompts_diffedit = {}\n",
        "\n",
        "# File paths for prompt files\n",
        "prompts_custom_file = Path(\"/content/GenAI-for-Visual-Synthesis/outputs/custom_method/prompts_custom.json\")\n",
        "prompts_diffedit_file = Path(\"/content/GenAI-for-Visual-Synthesis/outputs/diffedit/prompts_diffedit.json\")\n",
        "\n",
        "# Check if prompt files already exist and load them\n",
        "if prompts_custom_file.exists() and prompts_diffedit_file.exists():\n",
        "    print(\"ğŸ“‚ Found existing prompt files! Loading...\")\n",
        "\n",
        "    with open(prompts_custom_file, 'r') as f:\n",
        "        sample_prompts_custom = json.load(f)\n",
        "\n",
        "    with open(prompts_diffedit_file, 'r') as f:\n",
        "        sample_prompts_diffedit = json.load(f)\n",
        "\n",
        "    print(f\"âœ“ Loaded prompts for {len(sample_prompts_custom)} images from existing files\")\n",
        "    print(f\"  - {prompts_custom_file}\")\n",
        "    print(f\"  - {prompts_diffedit_file}\")\n",
        "\n",
        "    # Show first 3 prompts as preview\n",
        "    if len(sample_prompts_custom) > 0:\n",
        "        print(\"\\nğŸ“ Preview of loaded prompts (first 3):\")\n",
        "        for idx, (img_name, prompts) in enumerate(list(sample_prompts_custom.items())[:3], 1):\n",
        "            print(f\"\\n  {idx}. {img_name}\")\n",
        "            print(f\"     Vehicle: {prompts.get('vehicle', 'N/A')}\")\n",
        "            print(f\"     Background: {prompts.get('background', 'N/A')}\")\n",
        "\n",
        "        if len(sample_prompts_custom) > 3:\n",
        "            print(f\"\\n  ... and {len(sample_prompts_custom) - 3} more images\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ To regenerate prompts, delete the JSON files and run the auto-generation cell.\")\n",
        "\n",
        "# If no existing files, generate generic prompts as fallback\n",
        "elif len(image_files) > 0:\n",
        "    print(\"âŒ No existing prompt files found.\")\n",
        "    print(\"\\nâš ï¸  IMPORTANT: You have two options:\")\n",
        "    print(\"   1. Run the 'Auto-Generate Prompts with AI' cell below (RECOMMENDED)\")\n",
        "    print(\"   2. Continue with generic prompts (not recommended)\")\n",
        "    print(\"\\nCreating generic prompts as temporary fallback...\")\n",
        "\n",
        "    for img_file in image_files:\n",
        "        img_name = img_file.name\n",
        "        # Generic prompts - PLEASE CUSTOMIZE FOR YOUR IMAGES\n",
        "        sample_prompts_custom[img_name] = {\n",
        "            \"vehicle\": \"sleek modern sports car\",\n",
        "            \"background\": \"scenic highway at sunset\"\n",
        "        }\n",
        "        sample_prompts_diffedit[img_name] = {\n",
        "            \"source\": \"a car on the road\",\n",
        "            \"target\": \"a modern sports car\"\n",
        "        }\n",
        "\n",
        "    print(f\"âœ“ Created generic prompts for {len(image_files)} images\")\n",
        "    print(\"\\nâš ï¸  WARNING: Using GENERIC prompts - all images will have the same prompt!\")\n",
        "    print(\"   For better results, run the auto-generation cell below.\")\n",
        "\n",
        "    # Save generic prompts to files\n",
        "    with open(prompts_custom_file, 'w') as f:\n",
        "        json.dump(sample_prompts_custom, f, indent=2)\n",
        "\n",
        "    with open(prompts_diffedit_file, 'w') as f:\n",
        "        json.dump(sample_prompts_diffedit, f, indent=2)\n",
        "\n",
        "    print(f\"\\nâœ“ Generic prompts saved to:\")\n",
        "    print(f\"  - {prompts_custom_file}\")\n",
        "    print(f\"  - {prompts_diffedit_file}\")\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸  No image files found. Please add images to the test_data directory first.\")"
      ],
      "metadata": {
        "id": "-lm1ZKhS3PZX",
        "outputId": "dd12e9c0-ae10-4002-b3df-189203903517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-lm1ZKhS3PZX",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Found existing prompt files! Loading...\n",
            "âœ“ Loaded prompts for 75 images from existing files\n",
            "  - /content/GenAI-for-Visual-Synthesis/outputs/custom_method/prompts_custom.json\n",
            "  - /content/GenAI-for-Visual-Synthesis/outputs/diffedit/prompts_diffedit.json\n",
            "\n",
            "ğŸ“ Preview of loaded prompts (first 3):\n",
            "\n",
            "  1. 000aa097d423_03.jpg\n",
            "     Vehicle: sleek modern sports car\n",
            "     Background: scenic highway at sunset\n",
            "\n",
            "  2. 00ad56bf7ee6_03.jpg\n",
            "     Vehicle: sleek modern sports car\n",
            "     Background: scenic highway at sunset\n",
            "\n",
            "  3. 00afb946a54c_03.jpg\n",
            "     Vehicle: sleek modern sports car\n",
            "     Background: scenic highway at sunset\n",
            "\n",
            "  ... and 72 more images\n",
            "\n",
            "ğŸ’¡ To regenerate prompts, delete the JSON files and run the auto-generation cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb551106",
      "metadata": {
        "id": "bb551106"
      },
      "source": [
        "## ğŸš€ Step 2: Run Custom Method\n",
        "\n",
        "This will process all test images through your 4-stage pipeline:\n",
        "1. **Stage 1**: UNet segmentation (original image)\n",
        "2. **Stage 2**: Stable Diffusion vehicle regeneration\n",
        "3. **Stage 3**: UNet re-segmentation (edited image)\n",
        "4. **Stage 4**: Stable Diffusion background inpainting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r /content/GenAI-for-Visual-Synthesis/requirements.txt"
      ],
      "metadata": {
        "id": "A95uSJ7T4W6K",
        "outputId": "0f7176cb-ba8e-4974-e70c-8a7b05490964",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A95uSJ7T4W6K",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 20))\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-5_y7lblt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-5_y7lblt\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 2)) (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (0.35.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 6)) (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 7)) (1.11.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 8)) (0.6.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (0.121.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 11)) (0.0.20)\n",
            "Collecting torchmetrics (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 14))\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 15)) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 16)) (4.67.1)\n",
            "Collecting ftfy (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 17))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 18)) (2024.11.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (3.10.0)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.12/dist-packages (from -r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10)) (0.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (8.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (2.32.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 6)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 6)) (0.22.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (0.49.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (2.11.10)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10)) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10)) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10))\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10)) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10))\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10))\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 10)) (15.0.1)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 14))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 17)) (0.2.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (2.9.0.post0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 19)) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.50.0,>=0.40.0->fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (4.11.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 5)) (2025.10.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi->-r /content/GenAI-for-Visual-Synthesis/requirements.txt (line 9)) (1.3.1)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=96d611c1b39e1e701880bf368711e014a7b3c1b8478fd6002570795b24d89e0a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m2yz7umx/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: uvloop, lightning-utilities, httptools, ftfy, watchfiles, torchmetrics, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1 httptools-0.7.1 lightning-utilities-0.15.2 torchmetrics-1.8.2 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "72e61560",
      "metadata": {
        "id": "72e61560",
        "outputId": "3a0c7b9d-3fe1-4b5b-8b0d-4b6cd522215b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Imported pipeline functions from: /content/GenAI-for-Visual-Synthesis/main.py\n",
            "âœ“ Base directory: /content/GenAI-for-Visual-Synthesis\n",
            "\n",
            "Checking model files...\n",
            "âœ“ UNet model found: unet_model_carvana_new.pth\n",
            "âœ“ Stable Diffusion model found\n",
            "\n",
            "âœ“ All imports successful and ready to process images!\n"
          ]
        }
      ],
      "source": [
        "# Import custom pipeline functions\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path to import main.py\n",
        "MAIN_DIR = Path(\"/content/GenAI-for-Visual-Synthesis\")\n",
        "if str(MAIN_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(MAIN_DIR))\n",
        "\n",
        "# Import from main.py\n",
        "from main import (\n",
        "    segment_image,\n",
        "    regenerate_vehicle,\n",
        "    segment_edited_image,\n",
        "    inpaint_background,\n",
        "    BASE_DIR\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Imported pipeline functions from: {MAIN_DIR / 'main.py'}\")\n",
        "print(f\"âœ“ Base directory: {BASE_DIR}\")\n",
        "\n",
        "# Model paths\n",
        "unet_path = BASE_DIR / \"model\" / \"unet_model_carvana_new.pth\"\n",
        "sd_model_path = (\n",
        "    BASE_DIR / \"model\" / \"stable-diffusion\" /\n",
        "    \"models--runwayml--stable-diffusion-v1-5\" / \"snapshots\" /\n",
        "    \"451f4fe16113bff5a5d2269ed5ad43b0592e9a14\"\n",
        ")\n",
        "\n",
        "# Check models exist\n",
        "print(\"\\nChecking model files...\")\n",
        "if not unet_path.exists():\n",
        "    print(f\"âŒ UNet model not found: {unet_path}\")\n",
        "    print(\"   Please run setup.py to download models\")\n",
        "else:\n",
        "    print(f\"âœ“ UNet model found: {unet_path.name}\")\n",
        "\n",
        "if not sd_model_path.exists():\n",
        "    print(f\"âŒ Stable Diffusion model not found: {sd_model_path}\")\n",
        "    print(\"   Please run setup.py to download models\")\n",
        "else:\n",
        "    print(f\"âœ“ Stable Diffusion model found\")\n",
        "\n",
        "# Verify all imports are working\n",
        "print(\"\\nâœ“ All imports successful and ready to process images!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "21c9689e",
      "metadata": {
        "id": "21c9689e",
        "outputId": "dc02c034-2ae4-45f7-b5d3-747851a1bc05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504,
          "referenced_widgets": [
            "2f7d21e2d7d14de08bf39c1bd63f94f2",
            "74fd2be1c5b44e29b6b4efc70a2ba06e",
            "07dbcd152a55436f9dc19e09882c3f6e",
            "956d57ea1c2e436981de1271c49ee99e",
            "a5de2a5927ab46c8a63855cb6f7c6e66",
            "3ba0f8fb497d4c91bbdff4fe847c1d35",
            "1577b25b5b634cceb6b1346dd5bc17ce",
            "ddd0890401e8491798140e4b0f992a96",
            "6a3abad7a58f4cadbc949951d2e5c475",
            "576be049ecdb4c97ab107e1a0424dbd2",
            "20885869361748cf8de6fe5259fc4baf",
            "c749171c40734361ae004a951044d491",
            "eb3d536d5cd5447aa45a68375cd25b22",
            "be269b250f2442f4946c70e30bd7cb62",
            "5a84bd1cc3c14e2c84d44d10ff56955f",
            "0936e24b2e354395a4c6a114e88eef35",
            "3b603f15aa334e3eaa36e4677a6eb20a",
            "afa635837a6141fcba1106f2bd725051",
            "97442d09c2734abeaf5aa7e63e952ef5",
            "c782540ba7ef43c78975c20be1d8f349",
            "493bf9ba646a4dd29764b2eb27132e5d",
            "0bd3c132fc0e42e5a76e9d0771da73cc",
            "507e7272f4d84aeb866feed655605e3a",
            "710e7b260b14472ba12d2026ab8664d8",
            "d7d3d7500a7647ab8634bbc834813ef1",
            "738295f62dde4528a1ba455560e7bbf4",
            "4860326ac702437d8125f2e1ec6d4d07",
            "dab74f09888547f3a89990d1d85f5716",
            "24bb1c4eb24448199c703a17411a2646",
            "c769b6e88ea4464c9f34cc83bc0e0cef",
            "11d8893f27914d9c8c71b1b76557f494",
            "ace8d20809a34c9ba00108cefb14e41c",
            "6037ece16f704aafaec7250056375ea9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing images with Custom Method...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Custom Method:   0%|          | 0/75 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f7d21e2d7d14de08bf39c1bd63f94f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c749171c40734361ae004a951044d491"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "507e7272f4d84aeb866feed655605e3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2984921543.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Stage 2: Vehicle Regeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstage2_vehicle_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"stage2_vehicle_{img_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         _, stage2_vehicle_path = regenerate_vehicle(\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmask_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage1_mask_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GenAI-for-Visual-Synthesis/main.py\u001b[0m in \u001b[0;36mregenerate_vehicle\u001b[0;34m(img_path, mask_path, model_dir, prompt, negative_prompt, output_dir, output_name)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \"\"\"\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resolve_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, image, mask_image, masked_image_latents, height, width, padding_mask_crop, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m                 \u001b[0;31m# predict the noise residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m                 noise_pred = self.unet(\n\u001b[0m\u001b[1;32m   1274\u001b[0m                     \u001b[0mlatent_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsample_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupsample_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m                 sample = upsample_block(\n\u001b[0m\u001b[1;32m   1281\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m                 hidden_states = attn(\n\u001b[0m\u001b[1;32m   2459\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/transformers/transformer_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 )\n\u001b[1;32m    426\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 hidden_states = block(\n\u001b[0m\u001b[1;32m    428\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mff_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_chunked_feed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m             \u001b[0mff_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ada_norm_zero\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecation_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mdeprecation_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecation_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_torch_npu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# using torch_npu.npu_geglu can run faster and save memory on NPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Process images with custom method\n",
        "print(\"Processing images with Custom Method...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "custom_results = []\n",
        "\n",
        "for img_file in tqdm(image_files, desc=\"Custom Method\"):\n",
        "    img_name = img_file.name\n",
        "\n",
        "    if img_name not in sample_prompts_custom:\n",
        "        print(f\"âš  No prompts for {img_name}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    prompts = sample_prompts_custom[img_name]\n",
        "    vehicle_prompt = prompts.get('vehicle', '')\n",
        "    background_prompt = prompts.get('background', '')\n",
        "\n",
        "    try:\n",
        "        # Stage 1: Initial Segmentation\n",
        "        stage1_mask_name = f\"stage1_mask_{img_name}\"\n",
        "        stage1_mask_path = segment_image(\n",
        "            img_path=str(img_file),\n",
        "            model_path=str(unet_path),\n",
        "            output_dir=CUSTOM_OUTPUT,\n",
        "            output_name=stage1_mask_name\n",
        "        )\n",
        "\n",
        "        # Stage 2: Vehicle Regeneration\n",
        "        stage2_vehicle_name = f\"stage2_vehicle_{img_name}\"\n",
        "        _, stage2_vehicle_path = regenerate_vehicle(\n",
        "            img_path=str(img_file),\n",
        "            mask_path=stage1_mask_path,\n",
        "            model_dir=str(sd_model_path),\n",
        "            prompt=vehicle_prompt,\n",
        "            output_dir=CUSTOM_OUTPUT,\n",
        "            output_name=stage2_vehicle_name\n",
        "        )\n",
        "\n",
        "        # Stage 3: Re-segmentation\n",
        "        stage3_mask_name = f\"stage3_mask_{img_name}\"\n",
        "        stage3_mask_path = segment_edited_image(\n",
        "            img_path=stage2_vehicle_path,\n",
        "            model_path=str(unet_path),\n",
        "            output_dir=CUSTOM_OUTPUT,\n",
        "            output_name=stage3_mask_name\n",
        "        )\n",
        "\n",
        "        # Stage 4: Background Inpainting\n",
        "        stage4_final_name = f\"final_{img_name}\"\n",
        "        _, stage4_final_path = inpaint_background(\n",
        "            img_path=stage2_vehicle_path,\n",
        "            mask_path=stage3_mask_path,\n",
        "            model_dir=str(sd_model_path),\n",
        "            prompt=background_prompt,\n",
        "            output_dir=CUSTOM_OUTPUT,\n",
        "            output_name=stage4_final_name\n",
        "        )\n",
        "\n",
        "        custom_results.append({\n",
        "            'image': img_name,\n",
        "            'status': 'success',\n",
        "            'stage1_mask': stage1_mask_path,\n",
        "            'stage3_mask': stage3_mask_path,\n",
        "            'final': stage4_final_path\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error processing {img_name}: {e}\")\n",
        "        custom_results.append({\n",
        "            'image': img_name,\n",
        "            'status': 'error',\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "successful_custom = len([r for r in custom_results if r['status'] == 'success'])\n",
        "print(f\"\\nâœ“ Custom method completed: {successful_custom}/{len(image_files)} images successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b1d277",
      "metadata": {
        "id": "54b1d277"
      },
      "source": [
        "## ğŸ¨ Step 3: Run DiffEdit\n",
        "\n",
        "Now let's run DiffEdit on the same images for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884ff204",
      "metadata": {
        "id": "884ff204"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "# Setup DiffEdit pipeline\n",
        "print(\"Loading DiffEdit pipeline...\")\n",
        "diffedit_pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "\n",
        "diffedit_pipeline.scheduler = DDIMScheduler.from_config(diffedit_pipeline.scheduler.config)\n",
        "diffedit_pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(\n",
        "    diffedit_pipeline.scheduler.config\n",
        ")\n",
        "\n",
        "diffedit_pipeline.enable_model_cpu_offload()\n",
        "diffedit_pipeline.enable_vae_slicing()\n",
        "\n",
        "print(\"âœ“ DiffEdit pipeline loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a562d15b",
      "metadata": {
        "id": "a562d15b"
      },
      "outputs": [],
      "source": [
        "# Process images with DiffEdit\n",
        "print(\"\\nProcessing images with DiffEdit...\")\n",
        "print(\"=\"*60)\n",
        "print(\"Note: DiffEdit includes an inversion step, so each image takes longer (~60-120s)\")\n",
        "\n",
        "diffedit_results = []\n",
        "IMAGE_SIZE = (768, 768)\n",
        "\n",
        "for img_file in tqdm(image_files, desc=\"DiffEdit\"):\n",
        "    img_name = img_file.name\n",
        "\n",
        "    if img_name not in sample_prompts_diffedit:\n",
        "        print(f\"âš  No prompts for {img_name}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    prompts = sample_prompts_diffedit[img_name]\n",
        "    source_prompt = prompts.get('source', '')\n",
        "    target_prompt = prompts.get('target', '')\n",
        "\n",
        "    try:\n",
        "        # Load and resize image\n",
        "        raw_image = load_image(str(img_file)).resize(IMAGE_SIZE)\n",
        "\n",
        "        # Generate mask\n",
        "        mask_image = diffedit_pipeline.generate_mask(\n",
        "            image=raw_image,\n",
        "            source_prompt=source_prompt,\n",
        "            target_prompt=target_prompt,\n",
        "        )\n",
        "\n",
        "        # Invert latents\n",
        "        inv_latents = diffedit_pipeline.invert(\n",
        "            prompt=source_prompt,\n",
        "            image=raw_image\n",
        "        ).latents\n",
        "\n",
        "        # Generate final image\n",
        "        output_image = diffedit_pipeline(\n",
        "            prompt=target_prompt,\n",
        "            mask_image=mask_image,\n",
        "            image_latents=inv_latents,\n",
        "            negative_prompt=source_prompt,\n",
        "        ).images[0]\n",
        "\n",
        "        # Save outputs\n",
        "        output_path = DIFFEDIT_OUTPUT / f\"edited_{img_name}\"\n",
        "        output_image.save(output_path)\n",
        "\n",
        "        # Save mask\n",
        "        mask_pil = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\")\n",
        "        mask_pil = mask_pil.resize(IMAGE_SIZE)\n",
        "        mask_path = DIFFEDIT_OUTPUT / f\"mask_{img_name}\"\n",
        "        mask_pil.save(mask_path)\n",
        "\n",
        "        diffedit_results.append({\n",
        "            'image': img_name,\n",
        "            'status': 'success',\n",
        "            'output': str(output_path),\n",
        "            'mask': str(mask_path)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error processing {img_name}: {e}\")\n",
        "        diffedit_results.append({\n",
        "            'image': img_name,\n",
        "            'status': 'error',\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "successful_diffedit = len([r for r in diffedit_results if r['status'] == 'success'])\n",
        "print(f\"\\nâœ“ DiffEdit completed: {successful_diffedit}/{len(image_files)} images successful\")\n",
        "\n",
        "# Free memory\n",
        "del diffedit_pipeline\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bad656",
      "metadata": {
        "id": "59bad656"
      },
      "source": [
        "## ğŸ“Š Step 4: Compute Evaluation Metrics\n",
        "\n",
        "Now let's compute all the metrics to compare both methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c3ff98",
      "metadata": {
        "id": "73c3ff98"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
        "import clip\n",
        "from torchvision import transforms\n",
        "from scipy.stats import entropy\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "print(\"Initializing evaluation metrics...\")\n",
        "\n",
        "# Initialize metrics\n",
        "fid_metric = FrechetInceptionDistance(normalize=True).to(DEVICE)\n",
        "lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(DEVICE)\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
        "\n",
        "print(\"âœ“ Metrics initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9861f591",
      "metadata": {
        "id": "9861f591"
      },
      "outputs": [],
      "source": [
        "# Helper functions for metric computation\n",
        "\n",
        "def preprocess_for_fid(image_path):\n",
        "    \"\"\"Convert image to tensor for FID\"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_array = np.array(img)\n",
        "    if img_array.dtype != np.uint8:\n",
        "        img_array = (img_array * 255).astype(np.uint8)\n",
        "    tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
        "    return tensor.to(DEVICE)\n",
        "\n",
        "def preprocess_for_lpips(image_path):\n",
        "    \"\"\"Convert image to tensor for LPIPS\"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    return transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "def calculate_clip_similarity(image_path, text_prompt):\n",
        "    \"\"\"Calculate CLIP image-text similarity\"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    image_input = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n",
        "    text_input = clip.tokenize([text_prompt]).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_input)\n",
        "        text_features = clip_model.encode_text(text_input)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = (image_features @ text_features.T).item()\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def calculate_iou(mask1_path, mask2_path):\n",
        "    \"\"\"Calculate IoU between two masks\"\"\"\n",
        "    mask1 = np.array(Image.open(mask1_path).convert(\"L\")) > 127\n",
        "    mask2 = np.array(Image.open(mask2_path).convert(\"L\")) > 127\n",
        "\n",
        "    intersection = np.logical_and(mask1, mask2).sum()\n",
        "    union = np.logical_or(mask1, mask2).sum()\n",
        "\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return intersection / union\n",
        "\n",
        "print(\"âœ“ Helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1b3529",
      "metadata": {
        "id": "fd1b3529"
      },
      "outputs": [],
      "source": [
        "# Compute metrics for Custom Method\n",
        "print(\"Computing metrics for Custom Method...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "custom_metrics = {\n",
        "    'lpips_scores': [],\n",
        "    'clip_scores': [],\n",
        "    'iou_scores': [],\n",
        "    'per_image': []\n",
        "}\n",
        "\n",
        "# Reset FID\n",
        "fid_metric.reset()\n",
        "\n",
        "for result in tqdm(custom_results, desc=\"Custom Metrics\"):\n",
        "    if result['status'] != 'success':\n",
        "        continue\n",
        "\n",
        "    img_name = result['image']\n",
        "    orig_path = TEST_IMAGES_DIR / img_name\n",
        "    final_path = Path(result['final'])\n",
        "\n",
        "    # FID\n",
        "    orig_tensor = preprocess_for_fid(orig_path)\n",
        "    final_tensor = preprocess_for_fid(final_path)\n",
        "    fid_metric.update(orig_tensor, real=True)\n",
        "    fid_metric.update(final_tensor, real=False)\n",
        "\n",
        "    # LPIPS\n",
        "    orig_lpips = preprocess_for_lpips(orig_path)\n",
        "    final_lpips = preprocess_for_lpips(final_path)\n",
        "    lpips_score = lpips_metric(final_lpips, orig_lpips).item()\n",
        "    custom_metrics['lpips_scores'].append(lpips_score)\n",
        "\n",
        "    # CLIP (using target prompt)\n",
        "    if img_name in sample_prompts_custom:\n",
        "        prompts = sample_prompts_custom[img_name]\n",
        "        combined_prompt = f\"{prompts['vehicle']} on {prompts['background']}\"\n",
        "        clip_score = calculate_clip_similarity(final_path, combined_prompt)\n",
        "        custom_metrics['clip_scores'].append(clip_score)\n",
        "\n",
        "    # IoU (comparing stage 1 and stage 3 masks for consistency)\n",
        "    iou_score = calculate_iou(result['stage1_mask'], result['stage3_mask'])\n",
        "    custom_metrics['iou_scores'].append(iou_score)\n",
        "\n",
        "    custom_metrics['per_image'].append({\n",
        "        'image': img_name,\n",
        "        'lpips': lpips_score,\n",
        "        'clip': clip_score if img_name in sample_prompts_custom else None,\n",
        "        'iou': iou_score\n",
        "    })\n",
        "\n",
        "# Compute FID\n",
        "custom_metrics['fid'] = fid_metric.compute().item()\n",
        "custom_metrics['avg_lpips'] = np.mean(custom_metrics['lpips_scores'])\n",
        "custom_metrics['avg_clip'] = np.mean(custom_metrics['clip_scores'])\n",
        "custom_metrics['avg_iou'] = np.mean(custom_metrics['iou_scores'])\n",
        "\n",
        "print(f\"âœ“ Custom Method Metrics:\")\n",
        "print(f\"  FID:   {custom_metrics['fid']:.3f}\")\n",
        "print(f\"  LPIPS: {custom_metrics['avg_lpips']:.4f}\")\n",
        "print(f\"  CLIP:  {custom_metrics['avg_clip']:.4f}\")\n",
        "print(f\"  IoU:   {custom_metrics['avg_iou']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4a937b",
      "metadata": {
        "id": "8e4a937b"
      },
      "outputs": [],
      "source": [
        "# Compute metrics for DiffEdit\n",
        "print(\"\\nComputing metrics for DiffEdit...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "diffedit_metrics = {\n",
        "    'lpips_scores': [],\n",
        "    'clip_scores': [],\n",
        "    'iou_scores': [],\n",
        "    'per_image': []\n",
        "}\n",
        "\n",
        "# Reset FID\n",
        "fid_metric.reset()\n",
        "\n",
        "for result in tqdm(diffedit_results, desc=\"DiffEdit Metrics\"):\n",
        "    if result['status'] != 'success':\n",
        "        continue\n",
        "\n",
        "    img_name = result['image']\n",
        "    orig_path = TEST_IMAGES_DIR / img_name\n",
        "    output_path = Path(result['output'])\n",
        "\n",
        "    # FID\n",
        "    orig_tensor = preprocess_for_fid(orig_path)\n",
        "    output_tensor = preprocess_for_fid(output_path)\n",
        "    fid_metric.update(orig_tensor, real=True)\n",
        "    fid_metric.update(output_tensor, real=False)\n",
        "\n",
        "    # LPIPS\n",
        "    orig_lpips = preprocess_for_lpips(orig_path)\n",
        "    output_lpips = preprocess_for_lpips(output_path)\n",
        "    lpips_score = lpips_metric(output_lpips, orig_lpips).item()\n",
        "    diffedit_metrics['lpips_scores'].append(lpips_score)\n",
        "\n",
        "    # CLIP\n",
        "    if img_name in sample_prompts_diffedit:\n",
        "        target_prompt = sample_prompts_diffedit[img_name]['target']\n",
        "        clip_score = calculate_clip_similarity(output_path, target_prompt)\n",
        "        diffedit_metrics['clip_scores'].append(clip_score)\n",
        "\n",
        "    # IoU (compare with custom method's mask for the same image)\n",
        "    custom_result = next((r for r in custom_results if r['image'] == img_name), None)\n",
        "    if custom_result and custom_result['status'] == 'success':\n",
        "        iou_score = calculate_iou(result['mask'], custom_result['stage3_mask'])\n",
        "        diffedit_metrics['iou_scores'].append(iou_score)\n",
        "\n",
        "    diffedit_metrics['per_image'].append({\n",
        "        'image': img_name,\n",
        "        'lpips': lpips_score,\n",
        "        'clip': clip_score if img_name in sample_prompts_diffedit else None,\n",
        "        'iou': iou_score if custom_result else None\n",
        "    })\n",
        "\n",
        "# Compute FID\n",
        "diffedit_metrics['fid'] = fid_metric.compute().item()\n",
        "diffedit_metrics['avg_lpips'] = np.mean(diffedit_metrics['lpips_scores'])\n",
        "diffedit_metrics['avg_clip'] = np.mean(diffedit_metrics['clip_scores'])\n",
        "diffedit_metrics['avg_iou'] = np.mean(diffedit_metrics['iou_scores']) if diffedit_metrics['iou_scores'] else 0\n",
        "\n",
        "print(f\"âœ“ DiffEdit Metrics:\")\n",
        "print(f\"  FID:   {diffedit_metrics['fid']:.3f}\")\n",
        "print(f\"  LPIPS: {diffedit_metrics['avg_lpips']:.4f}\")\n",
        "print(f\"  CLIP:  {diffedit_metrics['avg_clip']:.4f}\")\n",
        "print(f\"  IoU:   {diffedit_metrics['avg_iou']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc3d5625",
      "metadata": {
        "id": "cc3d5625"
      },
      "source": [
        "## ğŸ“ˆ Step 5: Compare Results\n",
        "\n",
        "Let's create a comprehensive comparison of both methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9f6fd8",
      "metadata": {
        "id": "0e9f6fd8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Create comparison table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARATIVE RESULTS: Custom Method vs DiffEdit\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "comparison_data = {\n",
        "    'Metric': ['FID â†“', 'LPIPS â†“', 'CLIP â†‘', 'IoU â†‘'],\n",
        "    'Custom Method': [\n",
        "        f\"{custom_metrics['fid']:.3f}\",\n",
        "        f\"{custom_metrics['avg_lpips']:.4f}\",\n",
        "        f\"{custom_metrics['avg_clip']:.4f}\",\n",
        "        f\"{custom_metrics['avg_iou']:.4f}\"\n",
        "    ],\n",
        "    'DiffEdit': [\n",
        "        f\"{diffedit_metrics['fid']:.3f}\",\n",
        "        f\"{diffedit_metrics['avg_lpips']:.4f}\",\n",
        "        f\"{diffedit_metrics['avg_clip']:.4f}\",\n",
        "        f\"{diffedit_metrics['avg_iou']:.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Determine winners\n",
        "winners = []\n",
        "metrics_list = [\n",
        "    ('fid', False),  # lower is better\n",
        "    ('avg_lpips', False),\n",
        "    ('avg_clip', True),  # higher is better\n",
        "    ('avg_iou', True)\n",
        "]\n",
        "\n",
        "for metric_key, higher_better in metrics_list:\n",
        "    custom_val = custom_metrics[metric_key]\n",
        "    diffedit_val = diffedit_metrics[metric_key]\n",
        "\n",
        "    if higher_better:\n",
        "        winner = 'âœ“ Custom' if custom_val > diffedit_val else 'âœ“ DiffEdit'\n",
        "    else:\n",
        "        winner = 'âœ“ Custom' if custom_val < diffedit_val else 'âœ“ DiffEdit'\n",
        "\n",
        "    winners.append(winner)\n",
        "\n",
        "df['Winner'] = winners\n",
        "\n",
        "print(df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Count wins\n",
        "custom_wins = winners.count('âœ“ Custom')\n",
        "diffedit_wins = winners.count('âœ“ DiffEdit')\n",
        "\n",
        "print(f\"\\nOVERALL WINNER: \", end=\"\")\n",
        "if custom_wins > diffedit_wins:\n",
        "    print(\"âœ“ Custom Method\")\n",
        "elif diffedit_wins > custom_wins:\n",
        "    print(\"âœ“ DiffEdit\")\n",
        "else:\n",
        "    print(\"Tie\")\n",
        "\n",
        "print(f\"(Custom: {custom_wins} wins, DiffEdit: {diffedit_wins} wins)\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ea5c0f",
      "metadata": {
        "id": "d8ea5c0f"
      },
      "source": [
        "## ğŸ“Š Step 6: Visualize Results\n",
        "\n",
        "Let's create visual comparisons of the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d59883c9",
      "metadata": {
        "id": "d59883c9"
      },
      "outputs": [],
      "source": [
        "# Create bar chart comparison\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "metrics_to_plot = [\n",
        "    ('FID', 'fid', False),\n",
        "    ('LPIPS', 'avg_lpips', False),\n",
        "    ('CLIP', 'avg_clip', True),\n",
        "    ('IoU', 'avg_iou', True)\n",
        "]\n",
        "\n",
        "for idx, (name, key, higher_better) in enumerate(metrics_to_plot):\n",
        "    custom_val = custom_metrics[key]\n",
        "    diffedit_val = diffedit_metrics[key]\n",
        "\n",
        "    # Determine colors\n",
        "    if higher_better:\n",
        "        colors = ['green' if custom_val > diffedit_val else 'lightgreen',\n",
        "                 'blue' if diffedit_val > custom_val else 'lightblue']\n",
        "    else:\n",
        "        colors = ['green' if custom_val < diffedit_val else 'lightgreen',\n",
        "                 'blue' if diffedit_val < custom_val else 'lightblue']\n",
        "\n",
        "    bars = axes[idx].bar(['Custom', 'DiffEdit'],\n",
        "                        [custom_val, diffedit_val],\n",
        "                        color=colors)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.3f}',\n",
        "                      ha='center', va='bottom', fontsize=11)\n",
        "\n",
        "    # Styling\n",
        "    direction = 'â†‘ Higher Better' if higher_better else 'â†“ Lower Better'\n",
        "    axes[idx].set_title(f'{name}\\n{direction}', fontsize=13, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Score', fontsize=11)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Metrics Comparison: Custom Method vs DiffEdit',\n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ“ Metrics chart saved to: {OUTPUT_DIR / 'metrics_comparison.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e743288e",
      "metadata": {
        "id": "e743288e"
      },
      "outputs": [],
      "source": [
        "# Create side-by-side image comparisons\n",
        "print(\"\\nCreating visual comparisons of individual images...\")\n",
        "\n",
        "# Select up to 5 images to display\n",
        "display_images = min(5, len([r for r in custom_results if r['status'] == 'success']))\n",
        "\n",
        "fig, axes = plt.subplots(display_images, 3, figsize=(15, 5 * display_images))\n",
        "\n",
        "if display_images == 1:\n",
        "    axes = axes.reshape(1, -1)\n",
        "\n",
        "success_results = [(c, d) for c, d in zip(custom_results, diffedit_results)\n",
        "                   if c['status'] == 'success' and d['status'] == 'success']\n",
        "\n",
        "for idx, (custom_result, diffedit_result) in enumerate(success_results[:display_images]):\n",
        "    img_name = custom_result['image']\n",
        "\n",
        "    # Load images\n",
        "    original = Image.open(TEST_IMAGES_DIR / img_name)\n",
        "    custom_output = Image.open(custom_result['final'])\n",
        "    diffedit_output = Image.open(diffedit_result['output'])\n",
        "\n",
        "    # Display\n",
        "    axes[idx, 0].imshow(original)\n",
        "    axes[idx, 0].set_title(f'Original\\n{img_name}', fontsize=10)\n",
        "    axes[idx, 0].axis('off')\n",
        "\n",
        "    axes[idx, 1].imshow(custom_output)\n",
        "    axes[idx, 1].set_title('Custom Method', fontsize=10, color='green', fontweight='bold')\n",
        "    axes[idx, 1].axis('off')\n",
        "\n",
        "    axes[idx, 2].imshow(diffedit_output)\n",
        "    axes[idx, 2].set_title('DiffEdit', fontsize=10, color='blue', fontweight='bold')\n",
        "    axes[idx, 2].axis('off')\n",
        "\n",
        "plt.suptitle('Side-by-Side Comparison of Sample Images', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'visual_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ“ Visual comparison saved to: {OUTPUT_DIR / 'visual_comparison.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4497f7df",
      "metadata": {
        "id": "4497f7df"
      },
      "source": [
        "## ğŸ’¾ Step 7: Save Results\n",
        "\n",
        "Let's save all results to a JSON file for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb3c215",
      "metadata": {
        "id": "eeb3c215"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Compile all results\n",
        "final_results = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'test_configuration': {\n",
        "        'num_images': len(image_files),\n",
        "        'device': DEVICE,\n",
        "        'test_images_dir': str(TEST_IMAGES_DIR)\n",
        "    },\n",
        "    'custom_method': {\n",
        "        'fid': custom_metrics['fid'],\n",
        "        'lpips': custom_metrics['avg_lpips'],\n",
        "        'clip': custom_metrics['avg_clip'],\n",
        "        'iou': custom_metrics['avg_iou'],\n",
        "        'num_successful': len([r for r in custom_results if r['status'] == 'success'])\n",
        "    },\n",
        "    'diffedit_method': {\n",
        "        'fid': diffedit_metrics['fid'],\n",
        "        'lpips': diffedit_metrics['avg_lpips'],\n",
        "        'clip': diffedit_metrics['avg_clip'],\n",
        "        'iou': diffedit_metrics['avg_iou'],\n",
        "        'num_successful': len([r for r in diffedit_results if r['status'] == 'success'])\n",
        "    },\n",
        "    'per_image_comparison': []\n",
        "}\n",
        "\n",
        "# Add per-image details\n",
        "for custom_img, diffedit_img in zip(custom_metrics['per_image'], diffedit_metrics['per_image']):\n",
        "    final_results['per_image_comparison'].append({\n",
        "        'image': custom_img['image'],\n",
        "        'custom': custom_img,\n",
        "        'diffedit': diffedit_img\n",
        "    })\n",
        "\n",
        "# Save to file\n",
        "results_file = OUTPUT_DIR / 'evaluation_results.json'\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(f\"âœ“ Results saved to: {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5996bb4e",
      "metadata": {
        "id": "5996bb4e"
      },
      "source": [
        "## ğŸ¯ Summary & Interpretation\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "Your custom method should outperform DiffEdit if:\n",
        "- **FID is lower** â†’ Better image quality\n",
        "- **IoU is higher** â†’ Better segmentation accuracy\n",
        "- **CLIP is higher** â†’ Better text-prompt alignment\n",
        "\n",
        "### What the IoU Score Actually Means\n",
        "\n",
        "Since we don't have ground truth masks, the IoU here measures:\n",
        "1. **For Custom Method**: Consistency between Stage 1 and Stage 3 masks\n",
        "   - High IoU = the segmentation is stable/consistent\n",
        "2. **Between Methods**: How similar the masks are\n",
        "   - This shows if both methods identify similar regions\n",
        "\n",
        "### Understanding \"Good\" vs \"Bad\" Masks\n",
        "\n",
        "A good mask:\n",
        "- âœ“ Cleanly separates foreground from background\n",
        "- âœ“ Follows object boundaries accurately\n",
        "- âœ“ Leads to better final image quality (reflected in FID/CLIP)\n",
        "- âœ“ Is consistent across the pipeline\n",
        "\n",
        "A bad mask:\n",
        "- âœ— Has rough/jagged edges\n",
        "- âœ— Misses parts of the object or includes too much background\n",
        "- âœ— Causes artifacts in the final image\n",
        "- âœ— Is inconsistent between stages\n",
        "\n",
        "### Why Custom Method Should Win\n",
        "\n",
        "Your 4-stage approach has advantages:\n",
        "1. **Trained segmentation** (UNet) vs heuristic mask generation\n",
        "2. **Sequential refinement** (re-segment after editing)\n",
        "3. **Specialized control** (vehicle and background separately)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Review the visual comparisons above\n",
        "2. Check the JSON file for detailed per-image metrics\n",
        "3. Identify failure cases (if any)\n",
        "4. Fine-tune prompts for better results\n",
        "5. Use these results in your report/paper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb90437",
      "metadata": {
        "id": "dcb90437"
      },
      "source": [
        "## ğŸ“ Recommended Test Set Sizes\n",
        "\n",
        "Based on your evaluation needs:\n",
        "\n",
        "| Purpose | Images | Time (GPU) | Why |\n",
        "|---------|--------|------------|-----|\n",
        "| **Quick Test** | 5-10 | ~15-30 min | Verify pipeline works |\n",
        "| **Development** | 10-15 | ~30-45 min | Iterate on prompts |\n",
        "| **Evaluation** | 20-30 | ~1-2 hours | Reliable statistics |\n",
        "| **Publication** | 50-100 | ~3-5 hours | Publication-quality results |\n",
        "\n",
        "### Statistical Reliability\n",
        "\n",
        "- **< 10 images**: Results may be unreliable, high variance\n",
        "- **10-20 images**: Moderate confidence, good for class projects\n",
        "- **20-30 images**: Good confidence, suitable for papers\n",
        "- **50+ images**: High confidence, publication-quality\n",
        "\n",
        "The metrics become more stable with more images, especially FID and IS which measure distribution similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27416f10",
      "metadata": {
        "id": "27416f10"
      },
      "source": [
        "## âœ… Evaluation Complete!\n",
        "\n",
        "You've successfully:\n",
        "- âœ“ Run both methods on your test images\n",
        "- âœ“ Computed quantitative metrics (FID, LPIPS, CLIP, IoU)\n",
        "- âœ“ Created visual comparisons\n",
        "- âœ“ Saved all results to JSON\n",
        "\n",
        "Check the `outputs/` directory for all generated images and the results JSON file."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15f980ec1cf84beb8a4481155a9a2a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5070329658784a0ebfb59263222b1087",
              "IPY_MODEL_db6e181db10b45febf123bbdaf56617d",
              "IPY_MODEL_66f16b532933472e9e4636ad24c59d67"
            ],
            "layout": "IPY_MODEL_ef63956b708e4d519aa31270c8fae789"
          }
        },
        "5070329658784a0ebfb59263222b1087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a195976b17924046a7b7376126a885cc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f0c4182e34044597965742e4f421762b",
            "value": "Generatingâ€‡prompts:â€‡100%"
          }
        },
        "db6e181db10b45febf123bbdaf56617d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30e4227bcb0f44ee9a61570541c559ef",
            "max": 75,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88ca1406d3a240d8b98ce3b7c358763c",
            "value": 75
          }
        },
        "66f16b532933472e9e4636ad24c59d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bb1447fdcb943f18747e9ae9624ff42",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9b832cc55e5a46b3bbba049f9adfc0b0",
            "value": "â€‡75/75â€‡[02:30&lt;00:00,â€‡â€‡1.79s/it]"
          }
        },
        "ef63956b708e4d519aa31270c8fae789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a195976b17924046a7b7376126a885cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c4182e34044597965742e4f421762b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30e4227bcb0f44ee9a61570541c559ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ca1406d3a240d8b98ce3b7c358763c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bb1447fdcb943f18747e9ae9624ff42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b832cc55e5a46b3bbba049f9adfc0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f7d21e2d7d14de08bf39c1bd63f94f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74fd2be1c5b44e29b6b4efc70a2ba06e",
              "IPY_MODEL_07dbcd152a55436f9dc19e09882c3f6e",
              "IPY_MODEL_956d57ea1c2e436981de1271c49ee99e"
            ],
            "layout": "IPY_MODEL_a5de2a5927ab46c8a63855cb6f7c6e66"
          }
        },
        "74fd2be1c5b44e29b6b4efc70a2ba06e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba0f8fb497d4c91bbdff4fe847c1d35",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1577b25b5b634cceb6b1346dd5bc17ce",
            "value": "Customâ€‡Method:â€‡â€‡â€‡0%"
          }
        },
        "07dbcd152a55436f9dc19e09882c3f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd0890401e8491798140e4b0f992a96",
            "max": 75,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a3abad7a58f4cadbc949951d2e5c475",
            "value": 0
          }
        },
        "956d57ea1c2e436981de1271c49ee99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_576be049ecdb4c97ab107e1a0424dbd2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_20885869361748cf8de6fe5259fc4baf",
            "value": "â€‡0/75â€‡[00:09&lt;?,â€‡?it/s]"
          }
        },
        "a5de2a5927ab46c8a63855cb6f7c6e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba0f8fb497d4c91bbdff4fe847c1d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1577b25b5b634cceb6b1346dd5bc17ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddd0890401e8491798140e4b0f992a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3abad7a58f4cadbc949951d2e5c475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "576be049ecdb4c97ab107e1a0424dbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20885869361748cf8de6fe5259fc4baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c749171c40734361ae004a951044d491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb3d536d5cd5447aa45a68375cd25b22",
              "IPY_MODEL_be269b250f2442f4946c70e30bd7cb62",
              "IPY_MODEL_5a84bd1cc3c14e2c84d44d10ff56955f"
            ],
            "layout": "IPY_MODEL_0936e24b2e354395a4c6a114e88eef35"
          }
        },
        "eb3d536d5cd5447aa45a68375cd25b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b603f15aa334e3eaa36e4677a6eb20a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_afa635837a6141fcba1106f2bd725051",
            "value": "Loadingâ€‡pipelineâ€‡components...:â€‡100%"
          }
        },
        "be269b250f2442f4946c70e30bd7cb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97442d09c2734abeaf5aa7e63e952ef5",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c782540ba7ef43c78975c20be1d8f349",
            "value": 7
          }
        },
        "5a84bd1cc3c14e2c84d44d10ff56955f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493bf9ba646a4dd29764b2eb27132e5d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0bd3c132fc0e42e5a76e9d0771da73cc",
            "value": "â€‡7/7â€‡[00:00&lt;00:00,â€‡14.66it/s]"
          }
        },
        "0936e24b2e354395a4c6a114e88eef35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b603f15aa334e3eaa36e4677a6eb20a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa635837a6141fcba1106f2bd725051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97442d09c2734abeaf5aa7e63e952ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c782540ba7ef43c78975c20be1d8f349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "493bf9ba646a4dd29764b2eb27132e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd3c132fc0e42e5a76e9d0771da73cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507e7272f4d84aeb866feed655605e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_710e7b260b14472ba12d2026ab8664d8",
              "IPY_MODEL_d7d3d7500a7647ab8634bbc834813ef1",
              "IPY_MODEL_738295f62dde4528a1ba455560e7bbf4"
            ],
            "layout": "IPY_MODEL_4860326ac702437d8125f2e1ec6d4d07"
          }
        },
        "710e7b260b14472ba12d2026ab8664d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dab74f09888547f3a89990d1d85f5716",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_24bb1c4eb24448199c703a17411a2646",
            "value": "â€‡â€‡0%"
          }
        },
        "d7d3d7500a7647ab8634bbc834813ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c769b6e88ea4464c9f34cc83bc0e0cef",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11d8893f27914d9c8c71b1b76557f494",
            "value": 0
          }
        },
        "738295f62dde4528a1ba455560e7bbf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ace8d20809a34c9ba00108cefb14e41c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6037ece16f704aafaec7250056375ea9",
            "value": "â€‡0/50â€‡[00:02&lt;?,â€‡?it/s]"
          }
        },
        "4860326ac702437d8125f2e1ec6d4d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dab74f09888547f3a89990d1d85f5716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24bb1c4eb24448199c703a17411a2646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c769b6e88ea4464c9f34cc83bc0e0cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11d8893f27914d9c8c71b1b76557f494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ace8d20809a34c9ba00108cefb14e41c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6037ece16f704aafaec7250056375ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}