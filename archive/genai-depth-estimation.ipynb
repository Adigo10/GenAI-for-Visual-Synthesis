{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2002504,"sourceType":"datasetVersion","datasetId":1198025}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# =============== [1] Install Required Packages ==============\n!pip install scipy pillow matplotlib pandas --quiet\n\n# =============== [2] Imports ==============\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\nclass NYUDepthDatasetCSV(Dataset):\n    def __init__(self, base_folder, csv_file, transform_img=None, transform_depth=None):\n        self.base_folder = base_folder\n        self.data = pd.read_csv(csv_file, header=None)\n        self.transform_img = transform_img\n        self.transform_depth = transform_depth\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.base_folder, self.data.iloc[idx, 0])\n        depth_path = os.path.join(self.base_folder, self.data.iloc[idx, 1])\n        image = Image.open(img_path).convert('RGB')\n        depth = Image.open(depth_path).convert('L')\n        if self.transform_img:\n            image = self.transform_img(image)\n        if self.transform_depth:\n            depth = self.transform_depth(depth)\n        return image, depth\n\nbase_folder = \"/kaggle/input/nyu-depth-v2/nyu_data\"\ntrain_csv = os.path.join(base_folder, \"data/nyu2_train.csv\")\ntest_csv = os.path.join(base_folder, \"data/nyu2_test.csv\")\n\n\nimg_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ndepth_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n\ntrain_dataset = NYUDepthDatasetCSV(base_folder, train_csv, img_transform, depth_transform)\ntest_dataset = NYUDepthDatasetCSV(base_folder, test_csv, img_transform, depth_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T07:38:34.509181Z","iopub.execute_input":"2025-11-06T07:38:34.509691Z","iopub.status.idle":"2025-11-06T07:38:45.674366Z","shell.execute_reply.started":"2025-11-06T07:38:34.509656Z","shell.execute_reply":"2025-11-06T07:38:45.673517Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nprint(pd.read_csv(\"/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv\").columns)\nprint(pd.read_csv(\"/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv\").head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T07:38:45.675541Z","iopub.execute_input":"2025-11-06T07:38:45.675810Z","iopub.status.idle":"2025-11-06T07:38:45.851741Z","shell.execute_reply.started":"2025-11-06T07:38:45.675784Z","shell.execute_reply":"2025-11-06T07:38:45.850943Z"}},"outputs":[{"name":"stdout","text":"Index(['data/nyu2_train/living_room_0038_out/37.jpg', 'data/nyu2_train/living_room_0038_out/37.png'], dtype='object')\n    data/nyu2_train/living_room_0038_out/37.jpg  \\\n0  data/nyu2_train/living_room_0038_out/115.jpg   \n1    data/nyu2_train/living_room_0038_out/6.jpg   \n2   data/nyu2_train/living_room_0038_out/49.jpg   \n3  data/nyu2_train/living_room_0038_out/152.jpg   \n4  data/nyu2_train/living_room_0038_out/142.jpg   \n\n    data/nyu2_train/living_room_0038_out/37.png  \n0  data/nyu2_train/living_room_0038_out/115.png  \n1    data/nyu2_train/living_room_0038_out/6.png  \n2   data/nyu2_train/living_room_0038_out/49.png  \n3  data/nyu2_train/living_room_0038_out/152.png  \n4  data/nyu2_train/living_room_0038_out/142.png  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============== [5] Model Definition ==============\nclass DepthEstimationCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.enc1 = self.conv_block(3, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n        self.bottleneck = self.conv_block(512, 1024)\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.dec4 = self.conv_block(1024, 512)\n        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.dec3 = self.conv_block(512, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = self.conv_block(256, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = self.conv_block(128, 64)\n        self.out = nn.Conv2d(64, 1, 1)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(self.pool(enc1))\n        enc3 = self.enc3(self.pool(enc2))\n        enc4 = self.enc4(self.pool(enc3))\n        bottleneck = self.bottleneck(self.pool(enc4))\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat([dec4, enc4], dim=1)\n        dec4 = self.dec4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat([dec3, enc3], dim=1)\n        dec3 = self.dec3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat([dec2, enc2], dim=1)\n        dec2 = self.dec2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat([dec1, enc1], dim=1)\n        dec1 = self.dec1(dec1)\n        depth = self.out(dec1)\n        depth = torch.sigmoid(depth)\n        return depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T07:38:45.852592Z","iopub.execute_input":"2025-11-06T07:38:45.852978Z","iopub.status.idle":"2025-11-06T07:38:45.862280Z","shell.execute_reply.started":"2025-11-06T07:38:45.852951Z","shell.execute_reply":"2025-11-06T07:38:45.861608Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =============== [6] Multi-GPU Setup ==============\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = DepthEstimationCNN()\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\nimport time\n\ndef train_depth_model(\n    model, dataloader, criterion, optimizer, device, num_epochs=10, val_loader=None\n):\n    epoch_times = []\n    train_start = time.time()\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        epoch_start = time.time()\n        total_loss, total_mae, total_rmse, total_acc, total_samples = 0, 0, 0, 0, 0\n        for images, depths in dataloader:\n            images, depths = images.to(device), depths.to(device)\n            optimizer.zero_grad()\n            preds = model(images)\n            if preds.shape != depths.shape:\n                depths = nn.functional.interpolate(depths, size=preds.shape[2:], mode='bilinear', align_corners=True)\n            loss = criterion(preds, depths)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * images.size(0)\n            mae = torch.abs(preds - depths).mean().item()\n            rmse = torch.sqrt(((preds-depths)**2).mean()).item()\n            acc = ((torch.abs(preds - depths) < 0.1).float().mean().item())\n            total_mae += mae * images.size(0)\n            total_rmse += rmse * images.size(0)\n            total_acc += acc * images.size(0)\n            total_samples += images.size(0)\n        epoch_time = time.time() - epoch_start\n        epoch_times.append(epoch_time)\n        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n        elapsed = time.time() - train_start\n        remaining = avg_epoch_time * (num_epochs - epoch)\n        epoch_loss = total_loss / total_samples\n        epoch_mae = total_mae / total_samples\n        epoch_rmse = total_rmse / total_samples\n        epoch_acc = total_acc / total_samples\n        print(\n            f\"Epoch {epoch:02d} | Time: {epoch_time:.1f}s | \"\n            f\"Loss: {epoch_loss:.4f} | MAE: {epoch_mae:.4f} | \"\n            f\"RMSE: {epoch_rmse:.4f} | Acc(<0.1): {epoch_acc*100:.2f}% | \"\n            f\"Elapsed: {elapsed/60:.1f}m | ETA: {remaining/60:.1f}m\"\n        )\n\n        if val_loader:\n            val_mae, val_rmse, val_acc = evaluate_depth_model(model, val_loader, device)\n            print(\n                f\"  [VAL] MAE: {val_mae:.4f} | RMSE: {val_rmse:.4f} | Acc(<0.1): {val_acc*100:.2f}%\"\n            )\n\n\ndef evaluate_depth_model(model, dataloader, device):\n    model.eval()\n    total_mae, total_rmse, total_acc, total = 0., 0., 0., 0\n    with torch.no_grad():\n        for images, depths in dataloader:\n            images, depths = images.to(device), depths.to(device)\n            preds = model(images)\n            if preds.shape != depths.shape:\n                depths = nn.functional.interpolate(depths, size=preds.shape[2:], mode='bilinear', align_corners=True)\n            mae = torch.abs(preds - depths).mean().item()\n            rmse = torch.sqrt(((preds-depths)**2).mean()).item()\n            acc = ((torch.abs(preds - depths) < 0.1).float().mean().item())\n            total_mae += mae * images.size(0)\n            total_rmse += rmse * images.size(0)\n            total_acc += acc * images.size(0)\n            total += images.size(0)\n    return total_mae/total, total_rmse/total, total_acc/total\n\n\ndef visualize_batch(model, dataloader, device, num_images=4):\n    model.eval()\n    images, depths = next(iter(dataloader))\n    images, depths = images.to(device), depths.to(device)\n    with torch.no_grad():\n        preds = model(images)\n    img_vis = images[:num_images].cpu()\n    pred_vis = preds[:num_images].cpu()\n    depth_vis = depths[:num_images].cpu()\n    def show_grid(tensor, title):\n        grid_img = make_grid(tensor, nrow=num_images, normalize=True, scale_each=True)\n        np_img = grid_img.permute(1, 2, 0).numpy()\n        plt.figure(figsize=(12, 3))\n        plt.title(title)\n        plt.axis('off')\n        plt.imshow(np_img)\n        plt.show()\n    show_grid(img_vis, \"Input Images\")\n    show_grid(pred_vis, \"Predicted Depths\")\n    show_grid(depth_vis, \"Ground Truth Depths\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T07:38:45.863934Z","iopub.execute_input":"2025-11-06T07:38:45.864304Z","iopub.status.idle":"2025-11-06T07:38:46.402034Z","shell.execute_reply.started":"2025-11-06T07:38:45.864287Z","shell.execute_reply":"2025-11-06T07:38:46.401217Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ======================== [8] Run Training (Uncomment below to train) ========================\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.L1Loss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T07:38:46.402852Z","iopub.execute_input":"2025-11-06T07:38:46.403096Z","iopub.status.idle":"2025-11-06T07:38:46.407432Z","shell.execute_reply.started":"2025-11-06T07:38:46.403075Z","shell.execute_reply":"2025-11-06T07:38:46.406736Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_depth_model(model, train_loader, criterion, optimizer, device, num_epochs=10, val_loader=val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T07:38:46.408131Z","iopub.execute_input":"2025-11-06T07:38:46.408645Z","execution_failed":"2025-11-06T07:39:47.365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_batch(model, val_loader, device, num_images=4)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T07:39:47.365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport zipfile\n\n# ============ [1] Save the model AND optimizer checkpoint =============\nsave_dict = {\n    'model': model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict(),\n    'optimizer': optimizer.state_dict()\n}\nmodel_path = \"nyu_depth_model_with_optimizer.pth\"\ntorch.save(save_dict, model_path)\nprint(f\"Model+optimizer checkpoint saved as: {model_path}\")\n\n# ============ [2] Zip the checkpoint file =============\nzip_path = \"nyu_depth_model_with_optimizer.zip\"\nwith zipfile.ZipFile(zip_path, 'w') as zipf:\n    zipf.write(model_path)\n\nprint(f\"Checkpoint zipped as: {zip_path}\")\n\n# You can now download 'nyu_depth_model_with_optimizer.zip' from your notebook\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T07:39:47.365Z"}},"outputs":[],"execution_count":null}]}